<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> vLLM 源码解析（二） | HE Xin (贺鑫), a.k.a. MARSGGBO </title> <meta name="author" content="Xin He"> <meta name="description" content="" sss: study sleep slim> <meta name="keywords" content="PhD, AI, LLM, Neural Architecture Search, NAS, AutoML, Automated Machine Learning,HKBU, NUS, HUST"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-Block-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> HE Xin (贺鑫), a.k.a. MARSGGBO </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/others/">Others </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">vLLM 源码解析（二）</h1> <p class="post-meta"> February 04, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/serving"> <i class="fa-solid fa-hashtag fa-sm"></i> Serving</a>   <a href="/blog/tag/vllm"> <i class="fa-solid fa-hashtag fa-sm"></i> vLLM</a>   <a href="/blog/tag/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"> <i class="fa-solid fa-hashtag fa-sm"></i> 大模型推理</a>     ·   <a href="/blog/category/techniques"> <i class="fa-solid fa-tag fa-sm"></i> techniques</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="1-block-概览">1. Block 概览</h1> <p>vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 block 的关系。</p> <p>假设每个 block 可以用来存 4 个 token 的kv cache数据。一个句子的 token在逻辑层面是紧邻的，每次 decoding 生成新的 token 就往空闲的 block 里放。但是对应到物理层面的 block，一个句子的 token 可能分布在并不相邻的 block内，不过没关系，vLLM 会为每个句子的每个 token记录逻辑和物理block 的映射关系，方便查找和读取。</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/2024_3_23_1711187884361.png" alt="vLLM Block"></p> <p>接下来我们详细介绍 block 大小的含义，以及 block 的数量是如何计算的，最后介绍 vLLM 是如何管理 block 的。</p> <h1 id="2-block-大小如何计算">2. Block 大小如何计算</h1> <p>block 的大小可以自定义，上面定义为 4，简单理解就是每个 block 最多存储 4 个 token 的 kv cache 数据。但是 block 设置为 4 的时候对应到 GPU 内存到底是多大呢？其实这很好计算，</p> <p>一个 block 占用内存大小（Byte）= token 数量 (block_size) ✖️ 一个 token 的 kv cache 占用 内存大小。</p> <p>所以，我们只需要计算出单个 token 的 kv cache 对应的大小即可。block 大小的计算方法由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/cache_engine.py</code>文件里<code class="language-plaintext highlighter-rouge">CacheEngine</code>类的<code class="language-plaintext highlighter-rouge">get_cache_block_size</code>函数实现，代码也很简单，简化后如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/worker/cache_engine.py
</span><span class="k">class</span> <span class="nc">CacheEngine</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_cache_block_size</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span>
        <span class="n">parallel_config</span><span class="p">:</span> <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_head_size</span><span class="p">()</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_kv_heads</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_layers</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="n">key_cache_block</span> <span class="o">=</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span>
        <span class="n">value_cache_block</span> <span class="o">=</span> <span class="n">key_cache_block</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_cache_block</span> <span class="o">+</span> <span class="n">value_cache_block</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_dtype</span> <span class="o">==</span> <span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="n">dtype</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">STR_DTYPE_TO_TORCH_DTYPE</span><span class="p">[</span><span class="n">cache_dtype</span><span class="p">]</span>
        <span class="n">dtype_size</span> <span class="o">=</span> <span class="nf">_get_dtype_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dtype_size</span> <span class="o">*</span> <span class="n">total</span>
</code></pre></div></div> <p>上面代码中首先拿到 <code class="language-plaintext highlighter-rouge">num_heads</code>和<code class="language-plaintext highlighter-rouge">head_size</code>两个变量的值， <code class="language-plaintext highlighter-rouge">num_heads * head_size</code>就表示单个 token 在单层多头注意力机制计算中所需要的参数量，不过这只是 key 或者 value cache 所占用的参数量。</p> <p>一个 block 占用的内存 = token 数量（block_size）✖️ 层数 (num_layers) ✖️ 单层 kv cache 占用内存 （2✖️num_heads✖️head_size）✖️ 数据类型大小（如果是 fp16，则每个数据占用 2 Bytes）</p> <p>举例来说，假设 block_size=4， num_layers=4, num_heads=8, heads_size=128，采用 fp16 存储数据，那么</p> <p>一个 block 占用内存大小 = 4 ✖️ 4 ✖️ 8 ✖️ 128 ✖️ 2 = 32,768 Bytes。</p> <p>总结，一个 block 所占用的内存大小就是 block_size 个 token kv cache 所占内存的总和。不同模型的 block 各不相同。</p> <h1 id="2-block-数量如何计算">2. Block 数量如何计算</h1> <p>block 数量计算由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/worker.py</code>文件中<code class="language-plaintext highlighter-rouge">Worker</code>类的<code class="language-plaintext highlighter-rouge">profile_num_available_blocks</code>函数实现，该函数很简单，简化代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Worker</span>
    <span class="nd">@torch.inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">profile_num_available_blocks</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">cpu_swap_space</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
		
		<span class="c1"># 这一行其实就是用模拟数据跑一下forward 来统计GPU 的使用情况
</span>        <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">profile_run</span><span class="p">()</span>

        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
        <span class="n">free_gpu_memory</span><span class="p">,</span> <span class="n">total_gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">mem_get_info</span><span class="p">()</span>
        <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">total_gpu_memory</span> <span class="o">-</span> <span class="n">free_gpu_memory</span>

        <span class="n">cache_block_size</span> <span class="o">=</span> <span class="n">CacheEngine</span><span class="p">.</span><span class="nf">get_cache_block_size</span><span class="p">(</span>
            <span class="n">block_size</span><span class="p">,</span> <span class="n">cache_dtype</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span>
            <span class="p">(</span><span class="n">total_gpu_memory</span> <span class="o">*</span> <span class="n">gpu_memory_utilization</span> <span class="o">-</span> <span class="n">peak_memory</span><span class="p">)</span> <span class="o">//</span>
            <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">cpu_swap_space</span> <span class="o">//</span> <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_cpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="n">lora_manager</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">remove_all_loras</span><span class="p">()</span>
        <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="n">num_cpu_blocks</span>
</code></pre></div></div> <p>整个函数的逻辑很清晰，简单理解就是先用模拟数据跑一次 forward 记录下 GPU 的使用情况，这样可以知道 peak memory，然后计算每个 block 需要用到的 memory，接着就可以计算出 block 数量了。具体而言：</p> <ul> <li>13 行：vllm 默认用 256 个句子来做 profile，每个句子长度为 128</li> <li>15 到 17 行：统计 GPU 内存使用情况，返回的是以字节（Byte）为单位的数值，后面也都是基于 Byte 为单位进行计算的</li> <li>19 行：计算每个 block 的大小，这个在前面已经介绍。</li> <li>20-23 行：计算可用的 GPU block 数量。<code class="language-plaintext highlighter-rouge">num_gpu_blocks = int( (total_gpu_memory * gpu_memory_utilization - peak_memory) // cache_block_size)</code>：gpu_memory_utilization: 默认值是 0.9，表示 GPU 内存利用率是 90%，这挺高的了。所以最终的可用 GPU block 数量等于剩余 GPU 内存大小除以每个 block 的大小</li> <li>24 行：计算可用的 CPU block 数量。 <code class="language-plaintext highlighter-rouge">num_cpu_blocks = int(cpu_swap_space // cache_block_size)</code>这里的cpu_swap_space 代表每个 GPU 对应的 CPU swap 空间大小，单位是（GB），默认是是 4。也就是说每个 GPU 对应的 CPU swap 空间大小是 4 GB。</li> </ul> <h1 id="3-block-如何管理">3. Block 如何管理？</h1> <h2 id="31-逻辑-block-定义和使用">3.1 逻辑 Block 定义和使用</h2> <p>逻辑 Block（<code class="language-plaintext highlighter-rouge">LogicalTokenBlock</code>）定义如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">LogicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">_BLANK_TOKEN_ID</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">get_num_empty_slots</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>

    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span>

    <span class="k">def</span> <span class="nf">append_tokens</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
        <span class="n">curr_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">:</span><span class="n">curr_idx</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_token_ids</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[:</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_last_token_id</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <ul> <li>block_number: int: 这个是 PhysicalTokenBlock 实例对象的索引，可以理解成是 flag，用于区分不同 block</li> <li>block_size: int： 表示一个 block 内存储多少个 token 的 kv cache 数据。</li> <li> <code class="language-plaintext highlighter-rouge">__init__</code>函数中<code class="language-plaintext highlighter-rouge">self.token_ids</code>初始化是一个长度为 block_size 的全为 -1 的list。后续可以通过<code class="language-plaintext highlighter-rouge">append_tokens</code>将新的 token添加到这个 list 中去。</li> <li> <code class="language-plaintext highlighter-rouge">self.num_tokens</code>会统计已使用的 token 数量，当<code class="language-plaintext highlighter-rouge">self.num_tokens==block_size</code>时则表示这个 block 已经被使用完了。</li> </ul> <p>逻辑 Block 的使用逻辑是根据需要实时实例化一个对象，如果当前的 <code class="language-plaintext highlighter-rouge">LogicalBlock</code>没有剩余空间了，就再实例化一个新的。</p> <p>在 vLLm 的使用场景是在<code class="language-plaintext highlighter-rouge">vllm/vllm/sequence.py</code>里的<code class="language-plaintext highlighter-rouge">Sequence</code>类中根据需要动态创建<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Sequence</code>类在之前介绍 vLLM 的文章 【<a href="https://zhuanlan.zhihu.com/p/681402162" rel="external nofollow noopener" target="_blank">大模型推理框架 vLLM 源码解析（一）</a>】中已经有详细介绍，这里你只需要知道这个类记录了每个输入句子整个推理过程（prefilling 和 decoding）的所有信息。</p> </blockquote> <p>我们结合代码来看会更好理解，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/sequence.py
</span><span class="k">class</span> <span class="nc">Sequence</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span>
		<span class="bp">...</span>
		<span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LogicalTokenBlock</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">self</span><span class="p">.</span><span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span>
		<span class="bp">...</span>

    <span class="k">def</span> <span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">cursor</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>

            <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">is_full</span><span class="p">():</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>
                <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">num_empty_slots</span> <span class="o">=</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
            <span class="n">last_block</span><span class="p">.</span><span class="nf">append_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="n">cursor</span><span class="p">:</span><span class="n">cursor</span> <span class="o">+</span>
                                               <span class="n">num_empty_slots</span><span class="p">])</span>
            <span class="n">cursor</span> <span class="o">+=</span> <span class="n">num_empty_slots</span>
			
    <span class="k">def</span> <span class="nf">_append_logical_block</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">block</span> <span class="o">=</span> <span class="nc">LogicalTokenBlock</span><span class="p">(</span>
            <span class="n">block_number</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">),</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">__init__</code>函数中会初始化<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>空数组，用来存<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。可以看到会先将 prompt 的所有 token 通过<code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>存入到 block 中</li> <li> <code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>函数会遍历传入的 token_ids 数组中的每个 token id，将该 token 信息存入到 LogicalBlock 中。 <ul> <li>第 12 行：如果<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>为空，则会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个<code class="language-plaintext highlighter-rouge">LogicalBlock</code>，并存到<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>变量中去</li> <li>第 16 行：如果最新创建的<code class="language-plaintext highlighter-rouge">LogicalBlock</code>空间已经满了，则同样会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个新的<code class="language-plaintext highlighter-rouge">LogicalBlock</code> </li> </ul> </li> </ul> <h2 id="32-物理block-定义和管理">3.2 物理Block 定义和管理</h2> <p>物理 Block (<code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>)的代码定义如下：</p> <ul> <li>device: Device: 是一个 enum.Enum 实例对象，要么是 CPU 要么是 GPU。</li> <li>self.ref_count 变量用来指示这个 block 被使用的次数，默认为 0，代表没有使用。可以大于等于1，表示这个 block 内 token的 cache 被重复利用，使用场景比如可以是 beam search，这样可以重复利用cache，减少内存开销。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">PhysicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="nf">return </span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">PhysicalTokenBlock(device=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">block_number=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">block_number</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">ref_count=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">ref_count</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>只是针对单个 block 的描述。vLLM 在<code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>文件下实现了<code class="language-plaintext highlighter-rouge">BlockAllocator</code>类用来初始化所有物理 block，并负责分配这些 block。</p> <p><code class="language-plaintext highlighter-rouge">BlockAllocator</code>这个类代码很简单，如下。主要作用有三个：</p> <ul> <li> <code class="language-plaintext highlighter-rouge">__init__</code>: 初始化指定数量的物理层面 block，这个数量在前面一节已经介绍过如何计算。</li> <li> <code class="language-plaintext highlighter-rouge">allocate</code>: 通过 list的 pop() 函数返回一个可用的 block，并将该 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>设置为 1</li> <li> <code class="language-plaintext highlighter-rouge">free</code>：回收一个指定的 <code class="language-plaintext highlighter-rouge">PhysicalBlock</code>，但是回收的前提是这个 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>变量值为 0，表示这个 block 内的 token kv cache 数据不再需要了。 <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockAllocator</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
      <span class="n">self</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
      <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
      <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
      <span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span>

      <span class="c1"># Initialize the free blocks.
</span>      <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="nc">PhysicalTokenBlock</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                     <span class="n">block_number</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                                     <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PhysicalTokenBlock</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Out of memory! No free blocks are available.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">block</span>

  <span class="k">def</span> <span class="nf">free</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">PhysicalTokenBlock</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Double free! </span><span class="si">{</span><span class="n">block</span><span class="si">}</span><span class="s"> is already freed.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_num_free_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h2 id="33--block-管理和映射模块">3.3 Block 管理和映射模块</h2> <p>在介绍这个Block 管理模块之前，我们先了解 vLLM 中设置的用来判断句子是否能够被分配物理 Block 的三种状态，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">AllocStatus</span><span class="p">(</span><span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Result for BlockSpaceManager.can_allocate
    </span><span class="sh">"""</span>
    <span class="n">OK</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">LATER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">NEVER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
</code></pre></div></div> <p>三种状态的含义如下：</p> <ul> <li> <code class="language-plaintext highlighter-rouge">OK</code>: seq_group 可以现在被分配。</li> <li> <code class="language-plaintext highlighter-rouge">LATER</code>: seq_group 不能被分配。分配器的容量大于 seq_group 所需。</li> <li> <code class="language-plaintext highlighter-rouge">NEVER</code>: seq_group 永远不能被分配。seq_group 太大，无法在 GPU 中分配。</li> </ul> <p><code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>下的<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>是一个高级内存管理器，它在内存密集型计算任务（尤其是在使用GPU和CPU进行大规模数据处理的情况下）中管理逻辑数据块和物理内存块之间的映射。</p> <p>接下来，我们结合代码介绍<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>一些重要的函数。</p> <ul> <li>初始化函数<code class="language-plaintext highlighter-rouge">__init__</code>: <ul> <li> <code class="language-plaintext highlighter-rouge">watermark</code>: 一种阈值机制，用来决定何时停止在GPU上分配新的块，以避免内存不足</li> <li> <code class="language-plaintext highlighter-rouge">watermark_blocks</code>: 计算出在达到内存不足前，还能在GPU上分配多少个块。</li> <li> <code class="language-plaintext highlighter-rouge">sliding_window</code>: 可选参数，用来限制在任意给定时间内活跃的逻辑块的数量，有助于控制内存使用。</li> <li>创建了 cpu 和 gpu 两种 <code class="language-plaintext highlighter-rouge">BlockAllocator</code>,不过需要注意这里都是物理层面的 Block</li> <li>创建了一个字典 <code class="language-plaintext highlighter-rouge">block_tables</code>，用于存储每个 sequence id 和它所使用的物理块之间的映射。通过这个 sequence id ，我们就能找到对应的前面介绍的<code class="language-plaintext highlighter-rouge">Sequence</code>实例化对象，通过这个字典，就建立了逻辑 block 和物理 block 的映射关系。</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_gpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_cpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">watermark</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">=</span> <span class="n">num_gpu_blocks</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_cpu_blocks</span> <span class="o">=</span> <span class="n">num_cpu_blocks</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">sliding_window</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">sliding_window</span><span class="p">,</span>
                                                      <span class="n">block_size</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="n">sliding_window</span> <span class="o">//</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark</span> <span class="o">=</span> <span class="n">watermark</span>
        <span class="k">assert</span> <span class="n">watermark</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">watermark</span> <span class="o">*</span> <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">GPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_cpu_blocks</span><span class="p">)</span>
        <span class="c1"># Mapping: seq_id -&gt; BlockTable.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">BlockTable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">can_allocate</code> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="n">allocated</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">-=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="nf">get_num_blocks</span><span class="p">()</span>

      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">num_required_blocks</span><span class="p">,</span>
                                    <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span><span class="p">)</span>
      <span class="n">num_free_gpu_blocks</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">get_num_free_blocks</span><span class="p">()</span>

      <span class="c1"># Use watermark to avoid frequent cache eviction.
</span>      <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&lt;</span>
              <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">NEVER</span>
      <span class="k">if</span> <span class="n">num_free_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">OK</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">LATER</span>
</code></pre></div> </div> </li> </ul> <p><code class="language-plaintext highlighter-rouge">can_allocate</code>方法用于判断一个序列组（<code class="language-plaintext highlighter-rouge">seq_group</code>）是否能被成功分配所需的内存块。此方法首先计算该序列组基于当前任务的逻辑数据块所需的总物理内存块数量。接着，它会检查GPU分配器中的空闲内存块数量，以确认是否有足够的资源满足需求。</p> <p>方法中引入了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>概念，其主要目的是防止因频繁进行内存块的缓存淘汰而影响系统性能。在模型训练或数据处理的动态环境中，内存需求持续变化，如果因缺乏足够的空闲内存块而不得不频繁淘汰并重新分配内存块，将会造成性能损耗。这是因为被淘汰的内存块很可能很快再次需要使用，其重新分配过程会消耗额外的时间和资源。</p> <p>通过设置<code class="language-plaintext highlighter-rouge">watermark_blocks</code>阈值，当GPU上的空闲内存块数量低于此阈值时，系统将避免分配新的内存块，以留出缓冲区域，减少缓存淘汰的发生。只有当空闲内存块数量高于此阈值时，系统才会继续进行新的内存块分配。这种策略旨在平衡内存分配需求和系统性能，避免因频繁的内存操作而降低效率。</p> <p>如果根据当前的资源状态，确定序列组所需的内存块永远无法被满足，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.NEVER</code>，意味着该序列组在当前条件下无法被分配。如果当前不可分配但未来有可能，返回<code class="language-plaintext highlighter-rouge">AllocStatus.LATER</code>，表明序列组暂时无法分配，但随着系统状态的改变，可能在将来能够分配。如果有足够的空闲内存块满足分配需求，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.OK</code>，表示序列组可以立即被分配所需内存。</p> <p>这种方式确保了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>在满足内存分配需求的同时，有效避免了频繁的缓存淘汰问题，从而优化了整体的系统性能和资源利用效率。</p> <ul> <li> <code class="language-plaintext highlighter-rouge">allocate</code> 代码有简化，但是不影响理解 <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_prompt_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="n">block_table</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">logical_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_prompt_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">allocate</span><span class="p">()</span>
          <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">num_seqs</span><span class="p">()</span>
          <span class="n">block_table</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">):</span>
          <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">block_table</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
</code></pre></div> </div> <p><code class="language-plaintext highlighter-rouge">allocate</code> 方法用于为序列组分配内存块。它会遍历序列组中的每个序列，为每个序列分配足够的内存块，并将这些块添加到序列的块表中。同时，它会更新序列的块表，以便在后续的训练过程中可以正确地访问这些块。</p> </li> </ul> <p><code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>还有很多其它的函数，为了避免文章累赘，这里不做详细介绍。</p> <p>后面会继续写一篇 vLLM 的调度<code class="language-plaintext highlighter-rouge">Scheduler</code>模块的文章，对<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>更加详细地介绍。相信通过本篇文章，你应该能够对 vLLM 的 block 有一个清楚的了解了，如果还是不清楚，可以反复阅读直到清楚为止。</p> <h1 id="参考">参考</h1> <ul> <li>https://zhuanlan.zhihu.com/p/681018057</li> <li>https://zhuanlan.zhihu.com/p/656939628</li> <li>https://zhuanlan.zhihu.com/p/655561941</li> <li>https://zhuanlan.zhihu.com/p/658233994</li> <li>https://zhuanlan.zhihu.com/p/641999400</li> </ul> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br> <span>如有意合作或学术讨论欢迎私戳联系~<br>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br> </b> </h3> <p><b style="color:white;"></b> </p> </footer> </div> </article> </div> </div> </div> </div> <div class="clustrmaps-container" style="width: 70%; margin: 0 auto;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&amp;w=300&amp;t=tt&amp;d=MmPhDESko6sM29T5t949L9IK3zTIqI7QXYMuddg4Dos&amp;co=ffffff&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=808080"></script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Xin He. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: June 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>