<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pytorch 使用 storage 实现 offload 参数示例 | HE Xin (贺鑫), a.k.a. MARSGGBO </title> <meta name="author" content="Xin He"> <meta name="description" content="" sss: study sleep slim> <meta name="keywords" content="PhD, AI, LLM, Neural Architecture Search, NAS, AutoML, Automated Machine Learning,HKBU, NUS, HUST"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://marsggbo.github.io/blog/2024/Pytorch-%E4%BD%BF%E7%94%A8-storage-%E5%AE%9E%E7%8E%B0-offload-%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> HE Xin (贺鑫), a.k.a. MARSGGBO </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/others/">Others </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pytorch 使用 storage 实现 offload 参数示例</h1> <p class="post-meta"> April 10, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/%E6%8A%80%E6%9C%AF-pytorch-offload-torch-storage"> <i class="fa-solid fa-hashtag fa-sm"></i> 技术,pytorch,offload,torch.Storage</a>     ·   <a href="/blog/category/techniques"> <i class="fa-solid fa-tag fa-sm"></i> /techniques</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>在深入探讨 PyTorch 中的 <code class="language-plaintext highlighter-rouge">Storage</code> 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 <code class="language-plaintext highlighter-rouge">Storage</code>。</p> <h1 id="1-什么是-torchstorage">1. 什么是 <code class="language-plaintext highlighter-rouge">torch.Storage</code>?</h1> <p>在 PyTorch 中，<code class="language-plaintext highlighter-rouge">Storage</code> 是一种容纳数据的一维数组，它可以看作是一个底层的内存块，其中存储着特定类型的数据。与 <code class="language-plaintext highlighter-rouge">Tensor</code> 的关系非常紧密，实际上，每个 <code class="language-plaintext highlighter-rouge">Tensor</code> 都有一个与之关联的 <code class="language-plaintext highlighter-rouge">Storage</code> 对象。<code class="language-plaintext highlighter-rouge">Tensor</code> 提供了一个高维视图来操作存储在 <code class="language-plaintext highlighter-rouge">Storage</code> 中的数据。</p> <p><code class="language-plaintext highlighter-rouge">Storage</code> 的一个关键特性是它的数据排列是连续的，这使得数据可以迅速地在设备之间传输，例如从 CPU 到 GPU，省去了频繁索引的操作。此外，<code class="language-plaintext highlighter-rouge">Storage</code> 可以存在于不同的设备上，如 CPU 或 CUDA（GPU）。</p> <p>使用 storage 实现 offload 参数场景大致有如下：</p> <ul> <li> <p><strong>模型训练时的内存优化</strong>： 在深度学习模型训练过程中，特别是当使用的模型非常大，以至于单个 GPU 显存不足时，可以使用 offload 技术将部分数据暂时存储到 CPU 内存中，从而释放 GPU 显存用于计算。</p> </li> <li> <p><strong>数据预处理</strong>： 在进行大规模数据处理时，可以将不活跃的数据段 offload 到 CPU，以保持 GPU 资源用于执行高优先级的任务。</p> </li> <li> <p><strong>长期数据存储</strong>： 对于不需要频繁访问的大量数据，可以将其 offload 到 CPU 或其他存储系统，以减少昂贵的 GPU 存储资源的占用。</p> </li> </ul> <h1 id="2-理解-storage">2. 理解 <code class="language-plaintext highlighter-rouge">Storage</code> </h1> <h2 id="21-简单例子">2.1 简单例子</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>
</code></pre></div></div> <p>输出结果如下，可以看到打印出来的结果符合预期，有三个浮点数，storage 的类型是 <code class="language-plaintext highlighter-rouge">torch.storage.TypedStorage</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mf">0.0</span>
 <span class="mf">1.0</span>
 <span class="mf">2.0</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">TypedStorage</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <p>更一般地，我们还能打印看看无类型的 storage 是什么样的</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_storage</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="n">_untyped_storage</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下，可以看到总共有 12 个整数，这是因为前面我们使用的数据类型是 float32，也就是说每个数由 4 个字节（bytes）表示。因为 变量 x 总共有 3 个数，所有它的 storage 总共有 12 个字节。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>这些值实际上是浮点数<code class="language-plaintext highlighter-rouge">0</code>、<code class="language-plaintext highlighter-rouge">1</code>、<code class="language-plaintext highlighter-rouge">2</code>在内存中的字节级表示。需要注意的是，上面输出结果并不是随机值，而是这些浮点数在 IEEE 754 标准下的二进制表达。我们可以逐个解释这些值如何来的。</p> <h2 id="22-浮点数的-ieee-754-表示">2.2 浮点数的 IEEE 754 表示</h2> <p>对于类型 <code class="language-plaintext highlighter-rouge">float32</code>（即单精度浮点数），每个数字占用 4 个字节（32位），具体编码方式为：</p> <ul> <li>1 位符号位（最高位）</li> <li>8 位指数位</li> <li>23 位尾数位</li> </ul> <p>在解释这些值之前，我们先了解一下计算机中的 <strong>小端序（Little Endian）</strong> 存储方式：在这种存储方式中，低位字节存放在内存的低地址端，高位字节存放在高地址端。</p> <p>以<code class="language-plaintext highlighter-rouge">Tensor[0., 1., 2.]</code> 为例，我们来看看这些值在内存中是如何表示的：</p> <ol> <li> <strong>数字 0 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：全0（偏移量为127，因此全0表示指数-127）</li> <li>尾数位：全0</li> <li> <strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">00000000 00000000 00000000 00000000</code> </li> <li> <strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code> </li> <li> <strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code> </li> <li> <strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 0 0</code> </li> </ul> </li> <li> <strong>数字 1 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：127（偏移后为0，<code class="language-plaintext highlighter-rouge">01111111</code>）</li> <li>尾数位：全0（因为1.0的尾数部分无需额外存储）</li> <li> <strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">001111111 00000000000000000000000</code> </li> <li> <strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">3F 80 00 00</code> </li> <li> <strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 80 3F</code> </li> <li> <strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 128 63</code> (<code class="language-plaintext highlighter-rouge">80</code> 十六进制转十进制是 <code class="language-plaintext highlighter-rouge">128</code>，<code class="language-plaintext highlighter-rouge">3F</code> 转十进制是 <code class="language-plaintext highlighter-rouge">63</code>)</li> </ul> </li> <li> <strong>数字 2 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：128（偏移后为1，<code class="language-plaintext highlighter-rouge">10000000</code>）</li> <li>尾数位：全0（因为2.0的尾数部分也无需额外存储）</li> <li> <strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">010000000 00000000000000000000000</code> </li> <li> <strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">40 00 00 00</code> </li> <li> <strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 40</code> </li> </ul> </li> </ol> <h1 id="3-使用-storage-实现参数-offload-到-cpu">3. 使用 Storage 实现参数 offload 到 cpu</h1> <p>前面例子中的变量<code class="language-plaintext highlighter-rouge">x</code>在 cuda上，为了实现 offload，我们需要在 cpu 上创建一个 storage，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">nbytes</span><span class="p">).</span><span class="nf">pin_memory</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下,可以看到<code class="language-plaintext highlighter-rouge">offload_storage</code>是在 cpu 上，目前其上面的值都是一些随机值。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">208</span>
 <span class="mi">238</span>
 <span class="mi">22</span>
 <span class="mi">7</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">208</span>
 <span class="mi">66</span>
 <span class="mi">20</span>
 <span class="mi">6</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>接下来我们需要把 <code class="language-plaintext highlighter-rouge">x</code> offload 到 cpu 上，只需要对 storage 做 copy 操作即可，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>可以看到<code class="language-plaintext highlighter-rouge">x</code>的值被成功拷贝到 cpu 上，但是这离实现 offload 还有一步之遥，我们接下来继续看一个简单的 offload 例子。</p> <h1 id="4-gpu-参数-和-cpu-参数互换">4. gpu 参数 和 cpu 参数互换</h1> <p>我们接着将探讨如何利用 Storage 实现 GPU 和 CPU 之间的数据互换，这对于处理大型数据集或进行复杂的数据处理任务时尤其有用。</p> <p>假设我们有以下设置：</p> <ul> <li>一个 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 用于当前计算。</li> <li>多个 CPU <code class="language-plaintext highlighter-rouge">Storage</code> 用于存储额外的数据集，这些数据集可能在不同时间被需求到 GPU。</li> </ul> <h2 id="41--初始化环境">4.1 初始化环境</h2> <p>首先，我们定义一个在 CUDA 上的 Tensor 和多个在 CPU 上的 Storage，准备用于数据交换：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># 定义 CUDA Tensors (用于当前计算)
</span><span class="n">current_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># 定义 CPU Storages (用于存储额外数据)
</span><span class="n">extra_data1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Initial CUDA Tensor (Current Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">current_data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Initial CPU Storages (Extra Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 1:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <p>输出结果为：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Initial</span> <span class="n">CUDA</span> <span class="nc">Tensor </span><span class="p">(</span><span class="n">Current</span> <span class="n">Data</span><span class="p">):</span>
<span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>

<span class="n">Initial</span> <span class="n">CPU</span> <span class="nc">Storages </span><span class="p">(</span><span class="n">Extra</span> <span class="n">Data</span><span class="p">):</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="42-使用缓冲区进行数据交换">4.2 使用缓冲区进行数据交换</h2> <p>接下来，我们将根据需要将 CPU 上的数据加载到 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 中，同时将当前 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 的数据存储回某个 CPU <code class="language-plaintext highlighter-rouge">Storage</code>，这可以申请一个 buffer 来作为中间变量，反正数据丢失。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 缓冲区定义
</span><span class="n">cpu_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">size</span><span class="p">()).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>  <span class="c1"># CPU buffer storage
</span>
<span class="c1"># 场景1：将 current_data 保存到 extra_data1，从 extra_data1 加载新数据到 current_data
</span><span class="n">cpu_buffer</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>  <span class="c1"># Save current GPU data to CPU buffer
</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="nf">copy_</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">)</span>  <span class="c1"># Move from CUDA buffer to current_data
</span><span class="n">extra_data1</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">cpu_buffer</span><span class="p">)</span>  <span class="c1"># Move from CPU buffer to extra_data1 Storage
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">After Data Exchange Scenario 1:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Current Data on </span><span class="si">{</span><span class="n">current_data</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">current_data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Extra Data 1 on </span><span class="si">{</span><span class="n">extra_data1</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <h4 id="输出结果">输出结果</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">After</span> <span class="n">Data</span> <span class="n">Exchange</span> <span class="n">Scenario</span> <span class="mi">1</span><span class="p">:</span>
<span class="n">Updated</span> <span class="n">Current</span> <span class="n">Data</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span> <span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">Updated</span> <span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span> <span class="n">on</span> <span class="n">cpu</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <p>此示例清晰地展示了如何利用 PyTorch 的 Storage 类来有效管理内存资源，并通过使用 CPU 和 CUDA 缓冲区动态切换数据来优化应用性能。这种方法尤其适用于需要频繁在不同计算设备之间迁移数据的场景，从而保证计算效率和响应速度。</p> <p>尽管可以通过 PyTorch 的 to(‘cpu’) 或 to(‘cuda’) 方法简单地在设备间迁移数据，使用 Storage 提供了更细粒度的控制。这在处理需要大量连续物理存储空间的复杂模型时显得尤为重要。</p> <p>例如在混合专家模型（MoE）中，系统需要根据不同的请求动态调用不同的专家（模型）。每个专家可能包含的是由多层感知机 (MLP) 或更复杂结构组成的模型，其中每层的参数在内存中通常是不连续的。这种不连续性可能导致在将参数 offload 到 CPU 或重新加载到 GPU 时，因频繁的内存访问和索引操作而增加通信开销。</p> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br> <span>如有意合作或学术讨论欢迎私戳联系~<br>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br> </b> </h3> <p><b style="color:white;"></b> </p> </footer> </div> </article> </div> </div> </div> </div> <div class="clustrmaps-container" style="width: 70%; margin: 0 auto;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&amp;w=300&amp;t=tt&amp;d=MmPhDESko6sM29T5t949L9IK3zTIqI7QXYMuddg4Dos&amp;co=ffffff&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=808080"></script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Xin He. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>