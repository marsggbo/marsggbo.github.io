<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://marsggbo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://marsggbo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-21T11:06:43+00:00</updated><id>https://marsggbo.github.io/feed.xml</id><title type="html">MARSGGBO’s World</title><subtitle>&quot;SSS: Study, Sleep, Slim&quot; </subtitle><entry><title type="html">TACC 集群使用笔记</title><link href="https://marsggbo.github.io/blog/2024/TACC-%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/" rel="alternate" type="text/html" title="TACC 集群使用笔记"/><published>2024-04-10T16:40:16+00:00</published><updated>2024-04-10T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/TACC%20%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/TACC-%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"><![CDATA[<h1 id="1注册账号">1注册账号</h1> <p>现在网页上注册账号，之后需要联系导师或者管理员把你添加到对应的集群里去，例如我加入的是 Lonestar6 集群。</p> <p>之后需要跟着这个<a href="https://docs.tacc.utexas.edu/basics/mfa/">教程</a>绑定 MFA 软件（可以是 DUO 或者 1password）</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_5_1707138200561.png" alt="MFA 绑定"/></p> <p>之后登录账号,系统会要求先后输入你的账户密码和 MFA 的 6 位数 token</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">local</span>@username<span class="nv">$ </span>ssh username@ls6.tacc.utexas.edu
<span class="o">(</span>username@ls6.tacc.utexas.edu<span class="o">)</span> Password:
<span class="o">(</span>username@ls6.tacc.utexas.edu<span class="o">)</span> TACC Token Code:

login1.ls6<span class="o">(</span>22<span class="o">)</span><span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SCRATCH</span>/
login1.ls6<span class="o">(</span>23<span class="o">)</span><span class="err">$</span>
</code></pre></div></div> <p>密码都正确之后你会进入到 login 节点，在这里千万不能随意执行大规模的计算任务，因为很有可能会被封号。你需要使用 compute 节点执行计算任务。</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_5_1707139131743.png" alt="Login 和 Compute 节点"/></p> <p>成功登入后，默认进入 login 节点下的 <code class="language-plaintext highlighter-rouge">/home</code>目录，一般而言我们需要进入 <code class="language-plaintext highlighter-rouge">/scratch </code>目录。大多数TACC HPC资源上挂载了三个文件系统：<code class="language-plaintext highlighter-rouge">$HOME</code>、<code class="language-plaintext highlighter-rouge">$WORK</code>、和<code class="language-plaintext highlighter-rouge">$SCRATCH</code>，以下是它们的区别、使用场景和注意事项的总结：</p> <table> <thead> <tr> <th>文件系统</th> <th>区别与特点</th> <th>使用场景</th> <th>注意事项</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">$HOME</code></td> <td>用于用户的个人文件和设置。</td> <td>存储cron作业、小脚本、环境设置。</td> <td>避免在<code class="language-plaintext highlighter-rouge">$HOME</code>中运行作业，用于常规文件管理而不是并行作业。</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">$WORK</code></td> <td>用于存储软件安装、原始数据集等。</td> <td>存储软件安装、原始数据集、作业脚本和模板。</td> <td>注意文件系统配额，接近配额可能导致文件系统压力。</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">$SCRATCH</code></td> <td>临时存储、I/O文件、作业文件等。</td> <td>运行I/O密集型作业，存储临时数据集。</td> <td>避免在<code class="language-plaintext highlighter-rouge">$SCRATCH</code>中存储长期数据，文件可能在十天未访问后被清理。不要在<code class="language-plaintext highlighter-rouge">$SCRATCH</code>运行长期作业，用于短暂的、I/O密集型的作业。</td> </tr> </tbody> </table> <p>总体而言，<code class="language-plaintext highlighter-rouge">$HOME</code> 适用于个人文件和设置，<code class="language-plaintext highlighter-rouge">$WORK</code>适用于存储软件和重要数据，而<code class="language-plaintext highlighter-rouge">$SCRATCH</code>适用于短暂的、I/O密集型的作业。</p> <p>从实操性的角度说，进入computing node 后，默认先进入的是<code class="language-plaintext highlighter-rouge">HOME</code>目录，在这里你可以先设置好 conda 环境。接着，假如你要运行一个 pytorch 代码，你需要<code class="language-plaintext highlighter-rouge">cd $SCRATCH</code> 才能使用 GPU，这个目录下运行代码保存的日志是临时的，你需要将重要文件备份到 <code class="language-plaintext highlighter-rouge">$WORK</code> 目录下。</p> <h1 id="2-交互式开发环境-idev">2. 交互式开发环境 idev</h1> <p><code class="language-plaintext highlighter-rouge">idev</code> 是一个用于在TACC（Texas Advanced Computing Center）集群上创建交互式计算环境的命令行工具，可以在计算节点上创建一个交互式会话，可以在其中执行串行、OpenMP并行或MPI并行的代码，就像在批处理作业中一样。。以下是关于 <code class="language-plaintext highlighter-rouge">idev</code> 的一些主要用法和选项的介绍：</p> <h2 id="21-idev--参数选项">2.1 <code class="language-plaintext highlighter-rouge">idev</code> 参数选项：</h2> <ul> <li><code class="language-plaintext highlighter-rouge">-A account_name</code>：设置账户名称（默认为 <code class="language-plaintext highlighter-rouge">-A use_default</code>）。</li> <li><code class="language-plaintext highlighter-rouge">-m minutes</code>：设置计算时间（默认为 30 分钟）。</li> <li><code class="language-plaintext highlighter-rouge">-n total_tasks</code>：设置总任务数。</li> <li><code class="language-plaintext highlighter-rouge">-N nodes</code>：设置节点数量。</li> <li><code class="language-plaintext highlighter-rouge">-tpn tpn</code>：设置每节点任务数。</li> <li><code class="language-plaintext highlighter-rouge">-p queue_name</code>：设置队列名称（默认为 <code class="language-plaintext highlighter-rouge">-p development</code>）。</li> <li><code class="language-plaintext highlighter-rouge">-R</code>：查找用户的预约。</li> <li><code class="language-plaintext highlighter-rouge">-r reservation_name</code>：请求使用特定的预约。</li> <li><code class="language-plaintext highlighter-rouge">-r none</code>：禁用预约检查。</li> <li><code class="language-plaintext highlighter-rouge">-E</code>：在作业开始时通知。</li> <li><code class="language-plaintext highlighter-rouge">-e email_address</code>：在作业开始时通过指定的电子邮件地址通知。</li> <li><code class="language-plaintext highlighter-rouge">-t hh:mm:ss</code>：设置计算时间（默认为 30 分钟）。</li> <li><code class="language-plaintext highlighter-rouge">-queues</code>：列出系统的队列。</li> <li><code class="language-plaintext highlighter-rouge">-pselect</code>：显示可选择的 Slurm 队列。</li> <li><code class="language-plaintext highlighter-rouge">-qselect</code>：显示可选择的 Slurm 队列。</li> <li><code class="language-plaintext highlighter-rouge">-- &lt;other SLURM options&gt;</code>：必须在所有 idev 选项之后使用，用于指定其他 Slurm 选项。</li> </ul> <h2 id="22-示例">2.2 示例</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 默认设置：1 节点，16 任务，30 分钟，使用默认账户</span>
idev

<span class="c"># 显示帮助信息</span>
idev <span class="nt">--help</span>

<span class="c"># 设置队列、时间和账户</span>
idev <span class="nt">-p</span> development <span class="nt">-m</span> 90 <span class="nt">-A</span> AB-ccviss

<span class="c"># 设置队列、时间、账户、节点和任务数</span>
idev <span class="nt">-p</span> normal <span class="nt">-t</span> 00:90:00 <span class="nt">-A</span> TG-STA123 <span class="nt">-N</span> 2 <span class="nt">-n</span> 16

<span class="c"># 显示可选择的 Slurm 队列</span>
idev <span class="nt">-pselect</span>

<span class="c"># 设置交互式会话的最长时间为2小时, 1个节点，4 个任务，请求在 development 队列中执行计算任务</span>
idev <span class="nt">-t</span> 02:00:00 <span class="nt">-N</span> 1 <span class="nt">-n</span> 4 <span class="nt">-p</span> development

</code></pre></div></div> <p>上面最后一个例子使用的是名为<code class="language-plaintext highlighter-rouge">development</code>的节点，你也可以先使用<code class="language-plaintext highlighter-rouge">sinfo</code>命令查看所有节点，然后手动设置成空闲的节点，例如：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>sinfo
gpu-a100          up   infinite      1 drain<span class="k">*</span> c317-003
gpu-a100          up   infinite     67  alloc c302-[001-004],c303-[001-004],c305-[001-002,004],c306-[002-004],c308-[001-004],c309-[001-004],c310-[001-004],c315-[001,003-016],c316-[001-002,007-016],c317-[001-002,004-008,010-016]
gpu-a100          up   infinite      5   idle c304-001,c305-003,c306-001,c316-003,c317-009
gpu-a100-dev      up   infinite      2  alloc c301-[001,004]
<span class="nv">$ </span>idev <span class="nt">-t</span> 02:00:00 <span class="nt">-N</span> 1 <span class="nt">-n</span> 4 <span class="nt">-p</span> gpu-a100-dev
</code></pre></div></div> <p>上面命令会自动申请一个空闲的<code class="language-plaintext highlighter-rouge">gpu-a100-dev</code>节点。</p> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="TACC,"/><category term="集群"/><summary type="html"><![CDATA[1注册账号]]></summary></entry><entry><title type="html">Pytorch 使用 storage 实现 offload 参数示例</title><link href="https://marsggbo.github.io/blog/2024/Pytorch-%E4%BD%BF%E7%94%A8-storage-%E5%AE%9E%E7%8E%B0-offload-%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B/" rel="alternate" type="text/html" title="Pytorch 使用 storage 实现 offload 参数示例"/><published>2024-04-10T14:04:10+00:00</published><updated>2024-04-10T14:04:10+00:00</updated><id>https://marsggbo.github.io/blog/2024/Pytorch%20%E4%BD%BF%E7%94%A8%20storage%20%E5%AE%9E%E7%8E%B0%20offload%20%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/Pytorch-%E4%BD%BF%E7%94%A8-storage-%E5%AE%9E%E7%8E%B0-offload-%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B/"><![CDATA[<p>在深入探讨 PyTorch 中的 <code class="language-plaintext highlighter-rouge">Storage</code> 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 <code class="language-plaintext highlighter-rouge">Storage</code>。</p> <h1 id="1-什么是-torchstorage">1. 什么是 <code class="language-plaintext highlighter-rouge">torch.Storage</code>?</h1> <p>在 PyTorch 中，<code class="language-plaintext highlighter-rouge">Storage</code> 是一种容纳数据的一维数组，它可以看作是一个底层的内存块，其中存储着特定类型的数据。与 <code class="language-plaintext highlighter-rouge">Tensor</code> 的关系非常紧密，实际上，每个 <code class="language-plaintext highlighter-rouge">Tensor</code> 都有一个与之关联的 <code class="language-plaintext highlighter-rouge">Storage</code> 对象。<code class="language-plaintext highlighter-rouge">Tensor</code> 提供了一个高维视图来操作存储在 <code class="language-plaintext highlighter-rouge">Storage</code> 中的数据。</p> <p><code class="language-plaintext highlighter-rouge">Storage</code> 的一个关键特性是它的数据排列是连续的，这使得数据可以迅速地在设备之间传输，例如从 CPU 到 GPU，省去了频繁索引的操作。此外，<code class="language-plaintext highlighter-rouge">Storage</code> 可以存在于不同的设备上，如 CPU 或 CUDA（GPU）。</p> <p>使用 storage 实现 offload 参数场景大致有如下：</p> <ul> <li> <p><strong>模型训练时的内存优化</strong>： 在深度学习模型训练过程中，特别是当使用的模型非常大，以至于单个 GPU 显存不足时，可以使用 offload 技术将部分数据暂时存储到 CPU 内存中，从而释放 GPU 显存用于计算。</p> </li> <li> <p><strong>数据预处理</strong>： 在进行大规模数据处理时，可以将不活跃的数据段 offload 到 CPU，以保持 GPU 资源用于执行高优先级的任务。</p> </li> <li> <p><strong>长期数据存储</strong>： 对于不需要频繁访问的大量数据，可以将其 offload 到 CPU 或其他存储系统，以减少昂贵的 GPU 存储资源的占用。</p> </li> </ul> <h1 id="2-理解-storage">2. 理解 <code class="language-plaintext highlighter-rouge">Storage</code></h1> <h2 id="21-简单例子">2.1 简单例子</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>
</code></pre></div></div> <p>输出结果如下，可以看到打印出来的结果符合预期，有三个浮点数，storage 的类型是 <code class="language-plaintext highlighter-rouge">torch.storage.TypedStorage</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mf">0.0</span>
 <span class="mf">1.0</span>
 <span class="mf">2.0</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">TypedStorage</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <p>更一般地，我们还能打印看看无类型的 storage 是什么样的</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_storage</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="n">_untyped_storage</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下，可以看到总共有 12 个整数，这是因为前面我们使用的数据类型是 float32，也就是说每个数由 4 个字节（bytes）表示。因为 变量 x 总共有 3 个数，所有它的 storage 总共有 12 个字节。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>这些值实际上是浮点数<code class="language-plaintext highlighter-rouge">0</code>、<code class="language-plaintext highlighter-rouge">1</code>、<code class="language-plaintext highlighter-rouge">2</code>在内存中的字节级表示。需要注意的是，上面输出结果并不是随机值，而是这些浮点数在 IEEE 754 标准下的二进制表达。我们可以逐个解释这些值如何来的。</p> <h2 id="22-浮点数的-ieee-754-表示">2.2 浮点数的 IEEE 754 表示</h2> <p>对于类型 <code class="language-plaintext highlighter-rouge">float32</code>（即单精度浮点数），每个数字占用 4 个字节（32位），具体编码方式为：</p> <ul> <li>1 位符号位（最高位）</li> <li>8 位指数位</li> <li>23 位尾数位</li> </ul> <p>在解释这些值之前，我们先了解一下计算机中的 <strong>小端序（Little Endian）</strong> 存储方式：在这种存储方式中，低位字节存放在内存的低地址端，高位字节存放在高地址端。</p> <p>以<code class="language-plaintext highlighter-rouge">Tensor[0., 1., 2.]</code> 为例，我们来看看这些值在内存中是如何表示的：</p> <ol> <li><strong>数字 0 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：全0（偏移量为127，因此全0表示指数-127）</li> <li>尾数位：全0</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">00000000 00000000 00000000 00000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code></li> <li><strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 0 0</code></li> </ul> </li> <li><strong>数字 1 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：127（偏移后为0，<code class="language-plaintext highlighter-rouge">01111111</code>）</li> <li>尾数位：全0（因为1.0的尾数部分无需额外存储）</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">001111111 00000000000000000000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">3F 80 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 80 3F</code></li> <li><strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 128 63</code> (<code class="language-plaintext highlighter-rouge">80</code> 十六进制转十进制是 <code class="language-plaintext highlighter-rouge">128</code>，<code class="language-plaintext highlighter-rouge">3F</code> 转十进制是 <code class="language-plaintext highlighter-rouge">63</code>)</li> </ul> </li> <li><strong>数字 2 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：128（偏移后为1，<code class="language-plaintext highlighter-rouge">10000000</code>）</li> <li>尾数位：全0（因为2.0的尾数部分也无需额外存储）</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">010000000 00000000000000000000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">40 00 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 40</code></li> </ul> </li> </ol> <h1 id="3-使用-storage-实现参数-offload-到-cpu">3. 使用 Storage 实现参数 offload 到 cpu</h1> <p>前面例子中的变量<code class="language-plaintext highlighter-rouge">x</code>在 cuda上，为了实现 offload，我们需要在 cpu 上创建一个 storage，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">nbytes</span><span class="p">).</span><span class="nf">pin_memory</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下,可以看到<code class="language-plaintext highlighter-rouge">offload_storage</code>是在 cpu 上，目前其上面的值都是一些随机值。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">208</span>
 <span class="mi">238</span>
 <span class="mi">22</span>
 <span class="mi">7</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">208</span>
 <span class="mi">66</span>
 <span class="mi">20</span>
 <span class="mi">6</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>接下来我们需要把 <code class="language-plaintext highlighter-rouge">x</code> offload 到 cpu 上，只需要对 storage 做 copy 操作即可，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>可以看到<code class="language-plaintext highlighter-rouge">x</code>的值被成功拷贝到 cpu 上，但是这离实现 offload 还有一步之遥，我们接下来继续看一个简单的 offload 例子。</p> <h1 id="4-gpu-参数-和-cpu-参数互换">4. gpu 参数 和 cpu 参数互换</h1> <p>我们接着将探讨如何利用 Storage 实现 GPU 和 CPU 之间的数据互换，这对于处理大型数据集或进行复杂的数据处理任务时尤其有用。</p> <p>假设我们有以下设置：</p> <ul> <li>一个 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 用于当前计算。</li> <li>多个 CPU <code class="language-plaintext highlighter-rouge">Storage</code> 用于存储额外的数据集，这些数据集可能在不同时间被需求到 GPU。</li> </ul> <h2 id="41--初始化环境">4.1 初始化环境</h2> <p>首先，我们定义一个在 CUDA 上的 Tensor 和多个在 CPU 上的 Storage，准备用于数据交换：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># 定义 CUDA Tensors (用于当前计算)
</span><span class="n">current_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># 定义 CPU Storages (用于存储额外数据)
</span><span class="n">extra_data1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Initial CUDA Tensor (Current Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">current_data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Initial CPU Storages (Extra Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 1:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <p>输出结果为：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Initial</span> <span class="n">CUDA</span> <span class="nc">Tensor </span><span class="p">(</span><span class="n">Current</span> <span class="n">Data</span><span class="p">):</span>
<span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>

<span class="n">Initial</span> <span class="n">CPU</span> <span class="nc">Storages </span><span class="p">(</span><span class="n">Extra</span> <span class="n">Data</span><span class="p">):</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="42-使用缓冲区进行数据交换">4.2 使用缓冲区进行数据交换</h2> <p>接下来，我们将根据需要将 CPU 上的数据加载到 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 中，同时将当前 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 的数据存储回某个 CPU <code class="language-plaintext highlighter-rouge">Storage</code>，这可以申请一个 buffer 来作为中间变量，反正数据丢失。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 缓冲区定义
</span><span class="n">cpu_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">size</span><span class="p">()).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>  <span class="c1"># CPU buffer storage
</span>
<span class="c1"># 场景1：将 current_data 保存到 extra_data1，从 extra_data1 加载新数据到 current_data
</span><span class="n">cpu_buffer</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>  <span class="c1"># Save current GPU data to CPU buffer
</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="nf">copy_</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">)</span>  <span class="c1"># Move from CUDA buffer to current_data
</span><span class="n">extra_data1</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">cpu_buffer</span><span class="p">)</span>  <span class="c1"># Move from CPU buffer to extra_data1 Storage
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">After Data Exchange Scenario 1:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Current Data on </span><span class="si">{</span><span class="n">current_data</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">current_data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Extra Data 1 on </span><span class="si">{</span><span class="n">extra_data1</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <h4 id="输出结果">输出结果</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">After</span> <span class="n">Data</span> <span class="n">Exchange</span> <span class="n">Scenario</span> <span class="mi">1</span><span class="p">:</span>
<span class="n">Updated</span> <span class="n">Current</span> <span class="n">Data</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span> <span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">Updated</span> <span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span> <span class="n">on</span> <span class="n">cpu</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <p>此示例清晰地展示了如何利用 PyTorch 的 Storage 类来有效管理内存资源，并通过使用 CPU 和 CUDA 缓冲区动态切换数据来优化应用性能。这种方法尤其适用于需要频繁在不同计算设备之间迁移数据的场景，从而保证计算效率和响应速度。</p> <p>尽管可以通过 PyTorch 的 to(‘cpu’) 或 to(‘cuda’) 方法简单地在设备间迁移数据，使用 Storage 提供了更细粒度的控制。这在处理需要大量连续物理存储空间的复杂模型时显得尤为重要。</p> <p>例如在混合专家模型（MoE）中，系统需要根据不同的请求动态调用不同的专家（模型）。每个专家可能包含的是由多层感知机 (MLP) 或更复杂结构组成的模型，其中每层的参数在内存中通常是不连续的。这种不连续性可能导致在将参数 offload 到 CPU 或重新加载到 GPU 时，因频繁的内存访问和索引操作而增加通信开销。</p> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="/techniques"/><category term="技术,pytorch,offload,torch.Storage"/><summary type="html"><![CDATA[在深入探讨 PyTorch 中的 Storage 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 Storage。]]></summary></entry><entry><title type="html">TACC 集群使用笔记 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/tacc-marsggbo/" rel="alternate" type="text/html" title="TACC 集群使用笔记 - marsggbo"/><published>2024-04-10T06:26:00+00:00</published><updated>2024-04-10T06:26:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/tacc----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/tacc-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1注册账号 先在网页上注册账号，之后需要联系导师或者管理员把你添加到对应的集群里去，例如我加入的是 Lonestar6 集群。 之后需要跟着这个教程绑定 MFA 软件（可以是 DUO 或者 1password） 之后登录账号,系统会要求先后输入你的账户密码和 MFA 的 6 位数 token loc]]></summary></entry><entry><title type="html">图解 vLLM 的推理调度策略 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-marsggbo/" rel="alternate" type="text/html" title="图解 vLLM 的推理调度策略 - marsggbo"/><published>2024-04-04T02:32:00+00:00</published><updated>2024-04-04T02:32:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[原文： 从continuous batching到vLLM中的batching - 不知叫什么好的文章 - 知乎 https://zhuanlan.zhihu.com/p/688551989]]></summary></entry><entry><title type="html">大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/" rel="alternate" type="text/html" title="大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo"/><published>2024-03-23T12:48:00+00:00</published><updated>2024-03-23T12:48:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm-block----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. Block 概览 vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 bl]]></summary></entry><entry><title type="html">OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/" rel="alternate" type="text/html" title="OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo"/><published>2024-02-22T08:50:00+00:00</published><updated>2024-02-22T08:50:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/openai-soradiffusion---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[标题党一下，顺便蹭一下 OpenAI Sora大模型的热点，主要也是回顾一下扩散模型的原理。 1. 简单理解扩散模型 简单理解，扩散模型如下图所示可以分成两部分，一个是 forward，另一个是 reverse 过程： forward：这是加噪声的过程，表示为\(q(X_{0:T})\)，即在原图（]]></summary></entry><entry><title type="html">vLLM 源码解析（一）</title><link href="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/" rel="alternate" type="text/html" title="vLLM 源码解析（一）"/><published>2024-02-04T16:40:16+00:00</published><updated>2024-02-04T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/"><![CDATA[<h1 id="1-quick-start">1. Quick Start</h1> <p>创建如下代码，命名为 <code class="language-plaintext highlighter-rouge">run.py</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
	<span class="sh">"</span><span class="s">Have you followed marsggbo in Zhihu?</span><span class="sh">"</span><span class="p">,</span>
	<span class="sh">"</span><span class="s">你一键三连了吗？</span><span class="sh">"</span>
<span class="p">]</span> <span class="c1"># 输入prompts
</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># 采样策略
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">facebook/opt-125m</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 初始化 LLM
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span> <span class="c1"># 完成推理
</span><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
	<span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>执行命令：<code class="language-plaintext highlighter-rouge">python run.py</code>。该脚本会自动将模型以张量并行的方式在两个 GPU 上进行推理计算。</p> <p>整个推理过程大大致流程如下图所示，即</p> <ul> <li>1 给定一定数量的 prompts（字符串数组）</li> <li> <ol> <li>vllm 会使用 Scheduler 模块自动对需要推理句子进行调度</li> </ol> </li> <li> <ol> <li>根据调度的结果，使用 tokenizer 将字符串转换成 prompt id，然后喂给 model 进行计算得到 logits 预测结果</li> </ol> </li> <li> <ol> <li>根据 logits 预测结果和提前设置好的采样策略对结果进行采样得到新的 token id</li> </ol> </li> <li> <ol> <li>将采样结果保存到 output</li> </ol> </li> </ul> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707030203438.png" alt="inferencce pipeline"/></p> <h1 id="2-整体核心模块">2. 整体核心模块</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707037549078.png" alt="vllm 核心模块结构"/></p> <p>上图给出了 vLLM 核心模块之间的结构关系。接下来我们从简单的模块（即输入、采样和输出）开始介绍，最后详细介绍 LLM 模块。</p> <h1 id="3-sequence">3. Sequence</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707031824422.png" alt="句子模块"/></p> <p>如上图我们可以看到 vLLM 为输入的句子设计了很多子模块，这些模块的用处各不相同，但是有彼此之间有关系，下面分别详细介绍一下。</p> <h1 id="31-sequencestatus">3.1 SequenceStatus</h1> <p>首先看到 <code class="language-plaintext highlighter-rouge">SequenceStatus</code>，其源代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceStatus</span><span class="p">(</span><span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Status of a sequence.</span><span class="sh">"""</span>
    <span class="n">WAITING</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 等待中，句子还没开始推理，或者推理还未结束
</span>    <span class="n">RUNNING</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 运行中
</span>    <span class="n">SWAPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已交换
</span>    <span class="n">FINISHED_STOPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已停止
</span>    <span class="n">FINISHED_LENGTH_CAPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已长度限制
</span>    <span class="n">FINISHED_ABORTED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已中止
</span>    <span class="n">FINISHED_IGNORED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已忽略
</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_finished</span><span class="p">(</span><span class="n">status</span><span class="p">:</span> <span class="sh">"</span><span class="s">SequenceStatus</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># 判断状态是否为已停止、已长度限制、已中止或已忽略
</span>        <span class="k">return</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_STOPPED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_LENGTH_CAPPED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_ABORTED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_IGNORED</span><span class="p">,</span>
        <span class="p">]</span>
</code></pre></div></div> <h2 id="32-sequencedata">3.2 SequenceData</h2> <p><code class="language-plaintext highlighter-rouge">SequenceData</code> 用于存储与序列相关的数据。这个类有三个属性：<code class="language-plaintext highlighter-rouge">prompt_token_ids</code>（提示词的标记ID）、<code class="language-plaintext highlighter-rouge">output_token_ids</code>（生成文本的标记ID）和<code class="language-plaintext highlighter-rouge">cumulative_logprob</code>（累计对数概率）。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceData</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">prompt_token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cumulative_logprob</span> <span class="o">=</span> <span class="mf">0.0</span>
</code></pre></div></div> <h2 id="33-sequence">3.3 Sequence</h2> <p><code class="language-plaintext highlighter-rouge">Sequence</code> 用于存储序列的数据、状态和块信息,且每个序列有唯一标识，即<code class="language-plaintext highlighter-rouge">seq_id</code>。注意看下面的代码：</p> <ul> <li><strong>数据</strong>其实是通过上面的 <code class="language-plaintext highlighter-rouge">SequenceData</code> 保存的</li> <li>默认初始化状态，所有句子序列的<strong>状态</strong>都是 <code class="language-plaintext highlighter-rouge">SequenceStatus.WAITING</code></li> <li>所谓<strong>块信息</strong>，其实就是 vLLM 会在初始化阶段预留出一定数量的CPU 和 GPU 内存，一般是以 token 为单位的，例如在初始化的时候会使用值全为 0，大小为 (256, 128)的 prompt_ids做 warm up。每个序列会按照实际大小申请 block 来记录内存使用情况，即序列 token 数越多，属性<code class="language-plaintext highlighter-rouge">logical_token_blocks</code>包含的 block 个数也就越多。</li> <li> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sequence</span><span class="p">:</span>
<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">seq_id</span> <span class="o">=</span> <span class="n">seq_id</span>
    <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span>
    <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

    <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="nc">SequenceData</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="c1"># 数据
</span>
    <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LogicalTokenBlock</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Initialize the logical token blocks with the prompt token ids.
</span>    <span class="n">self</span><span class="p">.</span><span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="c1"># 块信息
</span>    <span class="n">self</span><span class="p">.</span><span class="n">status</span> <span class="o">=</span> <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span> <span class="c1"># 状态
</span>    <span class="bp">...</span>
</code></pre></div> </div> </li> </ul> <h2 id="33-sequencegroup">3.3 SequenceGroup</h2> <p><code class="language-plaintext highlighter-rouge">Sequence</code>只是单个序列的表示方式,<code class="language-plaintext highlighter-rouge">seq_id</code>是它的唯一标识。<code class="language-plaintext highlighter-rouge">SequenceGroup</code>则是为了表示多个序列，<code class="language-plaintext highlighter-rouge">request_id</code>是它的唯一标识，表示是第几个请求。</p> <p>具体而言，可以看到<code class="language-plaintext highlighter-rouge">__init__</code>函数有个参数是 <code class="language-plaintext highlighter-rouge">seqs: List[Sequence]</code>，它表示由一个或多个 Sequence 组成的列表，然后会通过<code class="language-plaintext highlighter-rouge">self.seqs_dict = {seq.seq_id: seq for seq in seqs}</code>转化成字典方便管理，这个字典的 key 是每个 Sequence 的唯一标识<code class="language-plaintext highlighter-rouge">seq_id</code>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceGroup</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Sequence</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">arrival_time</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Prefix</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seqs_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span><span class="p">:</span> <span class="n">seq</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seqs</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">sampling_params</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arrival_time</span> <span class="o">=</span> <span class="n">arrival_time</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>下面是 vLLm 中 LLMEngine 使用 Sequence 和 SequenceGroup 的场景示例：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LLMEngine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">add_request</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">arrival_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prefix_pos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode_request</span><span class="p">(</span>
            <span class="n">request_id</span><span class="o">=</span><span class="n">request_id</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">prompt_token_ids</span><span class="p">,</span>
            <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_request</span><span class="p">)</span> <span class="c1"># 将字符串序列转换成 id
</span>
        <span class="c1"># Create the sequences.
</span>        <span class="n">block_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cache_config</span><span class="p">.</span><span class="n">block_size</span>
        <span class="n">seq_id</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">seq_counter</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="nc">Sequence</span><span class="p">(</span><span class="n">seq_id</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                       <span class="n">lora_request</span><span class="p">)</span>

        <span class="c1"># Create the sequence group.
</span>        <span class="n">seq_group</span> <span class="o">=</span> <span class="nc">SequenceGroup</span><span class="p">(</span><span class="n">request_id</span><span class="p">,</span> <span class="p">[</span><span class="n">seq</span><span class="p">],</span> <span class="n">sampling_params</span><span class="p">,</span>
                                  <span class="n">arrival_time</span><span class="p">)</span>

        <span class="c1"># Add the sequence group to the scheduler.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">add_seq_group</span><span class="p">(</span><span class="n">seq_group</span><span class="p">)</span>
</code></pre></div></div> <p>可以看到<code class="language-plaintext highlighter-rouge">SequenceGroup</code>的<code class="language-plaintext highlighter-rouge">seqs</code>参数在最初阶段其实只是单个序列 ，即<code class="language-plaintext highlighter-rouge">[seq]</code>。但是我们知道其实一个 prompt 可以有多个输出结果，所以<code class="language-plaintext highlighter-rouge">SequenceGroup</code>的目的是管理一个输入 prompt的多个生成序列信息。如果我们设置<code class="language-plaintext highlighter-rouge">SamplingParams.n=2</code>（第 4 节会介绍），那么在推理过程中，<code class="language-plaintext highlighter-rouge">SequenceGroup</code>会新增一个 Sequence，这个新增的 Sequence 的 seq_id 和原来的那个 Sequence 不一样，具体的代码细节会在下一篇文章中介绍。</p> <h2 id="35-sequencegroupmetadata">3.5 SequenceGroupMetadata</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceGroupMetadata</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">is_prompt</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">seq_data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceData</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">is_prompt</span> <span class="o">=</span> <span class="n">is_prompt</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seq_data</span> <span class="o">=</span> <span class="n">seq_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">sampling_params</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span> <span class="o">=</span> <span class="n">block_tables</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>SequenceGroupMetadata 记录了一些元信息，下面代码展示了 Scheduler 模块是如何生成这些信息的：</p> <ul> <li><code class="language-plaintext highlighter-rouge">request_id</code> 就是 SequenceGroup的 request_id</li> <li><code class="language-plaintext highlighter-rouge">seq_data</code> 是一个字典，key 是每个 Sequence的 seq_id，value 则是对应的 data （即 SequenceData）</li> <li><code class="language-plaintext highlighter-rouge">block_tables</code>也是一个字典，key 也是每个 Sequence的 seq_id，value 这是对应 Sequence 申请的 block</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">schedule</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">],</span> <span class="n">SchedulerOutputs</span><span class="p">]:</span>
        <span class="n">scheduler_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_schedule</span><span class="p">()</span>

        <span class="c1"># Create input data structures.
</span>        <span class="n">seq_group_metadata_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq_group</span> <span class="ow">in</span> <span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">scheduled_seq_groups</span><span class="p">:</span>
            <span class="n">seq_data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceData</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">RUNNING</span><span class="p">):</span>
                <span class="n">seq_id</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span>
                <span class="n">seq_data</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">data</span> <span class="c1"># 单个 SequenceData
</span>                <span class="n">block_tables</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">block_manager</span><span class="p">.</span><span class="nf">get_block_table</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="c1"># 对应Sequence的block信息
</span>
            <span class="n">seq_group_metadata</span> <span class="o">=</span> <span class="nc">SequenceGroupMetadata</span><span class="p">(</span>
                <span class="n">request_id</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">request_id</span><span class="p">,</span>
                <span class="n">is_prompt</span><span class="o">=</span><span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">prompt_run</span><span class="p">,</span>
                <span class="n">seq_data</span><span class="o">=</span><span class="n">seq_data</span><span class="p">,</span>
                <span class="n">sampling_params</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">sampling_params</span><span class="p">,</span>
                <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
                <span class="n">lora_request</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">lora_request</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">seq_group_metadata_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">seq_group_metadata</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">seq_group_metadata_list</span><span class="p">,</span> <span class="n">scheduler_outputs</span>
</code></pre></div></div> <h2 id="36-sequenceoutput-和-sequencegroupoutput">3.6 SequenceOutput 和 SequenceGroupOutput</h2> <p>SequenceOutput 和 SequenceGroupOutput的关系就类似 Sequence 和 SequenceGroup。SequenceOutput其实就是记录了上一个 输入 token id 以及对应输出的 token id。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">parent_seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_token</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">parent_seq_id</span> <span class="o">=</span> <span class="n">parent_seq_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_token</span> <span class="o">=</span> <span class="n">output_token</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logprobs</span> <span class="o">=</span> <span class="n">logprobs</span>

<span class="k">class</span> <span class="nc">SequenceGroupOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">samples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceOutput</span><span class="p">],</span>
        <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PromptLogprobs</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_logprobs</span> <span class="o">=</span> <span class="n">prompt_logprobs</span>
</code></pre></div></div> <h1 id="4-samplingparams">4. SamplingParams</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707037767316.png" alt="SamplingParams"/></p> <p>SamplingParams 包含以下参数：</p> <ul> <li><code class="language-plaintext highlighter-rouge">n</code>：要生成的序列的数量，默认为 1。</li> <li><code class="language-plaintext highlighter-rouge">best_of</code>：从多少个序列中选择最佳序列，需要大于 n，默认等于 n。</li> <li><code class="language-plaintext highlighter-rouge">temperature</code>：用于控制生成结果的随机性，较低的温度会使生成结果更确定性，较高的温度会使生成结果更随机。</li> <li><code class="language-plaintext highlighter-rouge">top_p</code>：用于过滤掉生成词汇表中概率低于给定阈值的词汇，控制随机性。</li> <li><code class="language-plaintext highlighter-rouge">top_k</code>：选择前 k 个候选 token，控制多样性。</li> <li><code class="language-plaintext highlighter-rouge">presence_penalty</code>：用于控制生成结果中特定词汇的出现频率。</li> <li><code class="language-plaintext highlighter-rouge">frequency_penalty</code>：用于控制生成结果中词汇的频率分布。</li> <li><code class="language-plaintext highlighter-rouge">repetition_penalty</code>：用于控制生成结果中的词汇重复程度。</li> <li><code class="language-plaintext highlighter-rouge">use_beam_search</code>：是否使用束搜索来生成序列。</li> <li><code class="language-plaintext highlighter-rouge">length_penalty</code>：用于控制生成结果的长度分布。</li> <li><code class="language-plaintext highlighter-rouge">early_stopping</code>：是否在生成过程中提前停止。</li> <li><code class="language-plaintext highlighter-rouge">stop</code>：要停止生成的词汇列表。</li> <li><code class="language-plaintext highlighter-rouge">stop_token_ids</code>：要停止生成的词汇的ID列表。</li> <li><code class="language-plaintext highlighter-rouge">include_stop_str_in_output</code>：是否在输出结果中包含停止字符串。</li> <li><code class="language-plaintext highlighter-rouge">ignore_eos</code>：在生成过程中是否忽略结束符号。</li> <li><code class="language-plaintext highlighter-rouge">max_tokens</code>：生成序列的最大长度。</li> <li><code class="language-plaintext highlighter-rouge">logprobs</code>：用于记录生成过程的概率信息。</li> <li><code class="language-plaintext highlighter-rouge">prompt_logprobs</code>：用于记录生成过程的概率信息，用于特定提示。</li> <li><code class="language-plaintext highlighter-rouge">skip_special_tokens</code>：是否跳过特殊符号。</li> <li><code class="language-plaintext highlighter-rouge">spaces_between_special_tokens</code>：是否在特殊符号之间添加空格。</li> </ul> <p>这些参数的设置通常取决于具体需求和模型性能。以下是一些常见的设置指导方法：</p> <ul> <li><code class="language-plaintext highlighter-rouge">temperature</code>：较低的温度（如0.2）会产生更确定性的结果，而较高的温度（如0.8）会产生更随机的结果。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">presence_penalty、frequency_penalty 和 repetition_penalty</code>：这些参数可以用于控制生成结果中的词汇分布和重复程度。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">use_beam_search</code>：束搜索通常用于生成更高质量的结果，但可能会降低生成速度。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">length_penalty</code>：这个参数可以用于控制生成结果的长度。较高的值会产生更长的结果，而较低的值会产生更短的结果。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">early_stopping</code>：如果您不希望生成过长的结果，可以设置此参数为True。</li> <li><code class="language-plaintext highlighter-rouge">stop 和 stop_token_ids</code>：您可以使用这些参数来指定生成结果的结束条件。</li> </ul> <h1 id="5-output-模块">5. Output 模块</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707040962845.png" alt="Output模块"/></p> <p>Output 主要用于表示语言模型（LLM）的生成结果，包含如下两个模块：</p> <ul> <li><code class="language-plaintext highlighter-rouge">CompletionOutput</code></li> <li><code class="language-plaintext highlighter-rouge">RequestOutput</code></li> </ul> <p>通过上面的介绍我们知道一个 request 可能包含多个序列，<code class="language-plaintext highlighter-rouge">CompletionOutput</code> 用来表示一个 request 中某个序列的完整输出的数据，其中下面的<code class="language-plaintext highlighter-rouge">index</code>就表示该序列在 request 中的索引位置</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CompletionOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># 输出结果在请求中的索引
</span>        <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="c1"># 生成的文本
</span>        <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="c1"># 生成的文本对应的 token ID 列表
</span>        <span class="n">cumulative_logprob</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleLogprobs</span><span class="p">],</span>
        <span class="n">finish_reason</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># 序列完成的原因（SequenceStatus）
</span>        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
        <span class="n">self</span><span class="p">.</span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">finish_reason</span> <span class="o">=</span> <span class="n">finish_reason</span>
		<span class="bp">...</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">RequestOutput</code>则表示 request 所有序列的输出结果，有它的初始化函数可以看到它记录了对应的 <code class="language-plaintext highlighter-rouge">request_id</code>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RequestOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PromptLogprobs</span><span class="p">],</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompletionOutput</span><span class="p">],</span>
        <span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">prompt_token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">self</span><span class="p">.</span><span class="n">finished</span> <span class="o">=</span> <span class="n">finished</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>我们看看RequestOutput的from_seq_group就能很好理解<code class="language-plaintext highlighter-rouge">CompletionOutput</code>和 <code class="language-plaintext highlighter-rouge">RequestOutput</code>是如何使用的了。为方便理解，代码有删减，但是不影响最终结果：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RequestOutput</span><span class="p">:</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_seq_group</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">RequestOutput</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># 1. Get the top-n sequences.
</span>        <span class="n">n</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">sampling_params</span><span class="p">.</span><span class="n">n</span> <span class="c1"># 每个序列返回的生成序列数量
</span>        <span class="n">seqs</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">()</span>
		<span class="c1"># 根据累积 logprob 值来选择出前 n 个生成序列
</span>		<span class="n">sorting_key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">seq</span><span class="p">:</span> <span class="n">seq</span><span class="p">.</span><span class="nf">get_cumulative_logprob</span><span class="p">()</span>
        <span class="n">sorted_seqs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">sorting_key</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">top_n_seqs</span> <span class="o">=</span> <span class="n">sorted_seqs</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>

        <span class="c1"># 2. Create the outputs.
</span>        <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompletionOutput</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">top_n_seqs</span><span class="p">:</span>
            <span class="n">logprobs</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">output_logprobs</span>
            <span class="n">finshed_reason</span> <span class="o">=</span> <span class="n">SequenceStatus</span><span class="p">.</span><span class="nf">get_finished_reason</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">status</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nc">CompletionOutput</span><span class="p">(</span><span class="n">seqs</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="n">output_text</span><span class="p">,</span>
                                      <span class="n">seq</span><span class="p">.</span><span class="nf">get_output_token_ids</span><span class="p">(),</span>
                                      <span class="n">seq</span><span class="p">.</span><span class="nf">get_cumulative_logprob</span><span class="p">(),</span> <span class="n">logprobs</span><span class="p">,</span>
                                      <span class="n">finshed_reason</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Every sequence in the sequence group should have the same prompt.
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt_token_ids</span>
        <span class="n">prompt_logprobs</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt_logprobs</span>
        <span class="n">finished</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">is_finished</span><span class="p">()</span>
        <span class="k">return</span> <span class="nf">cls</span><span class="p">(</span><span class="n">seq_group</span><span class="p">.</span><span class="n">request_id</span><span class="p">,</span>
                   <span class="n">prompt</span><span class="p">,</span>
                   <span class="n">prompt_token_ids</span><span class="p">,</span>
                   <span class="n">prompt_logprobs</span><span class="p">,</span>
                   <span class="n">outputs</span><span class="p">,</span>
                   <span class="n">finished</span><span class="p">,</span>
                   <span class="n">lora_request</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">lora_request</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">RequestOutput</code>是通过对传入的<code class="language-plaintext highlighter-rouge">seq_group: SequenceGroup</code>进行解析后得到的。解析过程主要有两个阶段：</p> <ul> <li> <ol> <li>Get the top-n sequences：这一阶段就是对生成序列按照 cumulative_logprob 进行排序，最后选择出top-n 序列。</li> </ol> </li> <li> <ol> <li>Create the outputs：将所有top-n生成序列分别转换成 <code class="language-plaintext highlighter-rouge">CompletionOutput</code>列表，并作为<code class="language-plaintext highlighter-rouge">RequestOutput</code>的初始化参数。</li> </ol> </li> </ul> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="LLM"/><category term="Serving"/><category term="vLLM"/><category term="大模型推理"/><summary type="html"><![CDATA[1. Quick Start]]></summary></entry><entry><title type="html">vLLM 源码解析（二）</title><link href="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-Block-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86/" rel="alternate" type="text/html" title="vLLM 源码解析（二）"/><published>2024-02-04T16:40:16+00:00</published><updated>2024-02-04T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%20Block%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-Block-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86/"><![CDATA[<h1 id="1-block-概览">1. Block 概览</h1> <p>vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 block 的关系。</p> <p>假设每个 block 可以用来存 4 个 token 的kv cache数据。一个句子的 token在逻辑层面是紧邻的，每次 decoding 生成新的 token 就往空闲的 block 里放。但是对应到物理层面的 block，一个句子的 token 可能分布在并不相邻的 block内，不过没关系，vLLM 会为每个句子的每个 token记录逻辑和物理block 的映射关系，方便查找和读取。</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_3_23_1711187884361.png" alt="vLLM Block"/></p> <p>接下来我们详细介绍 block 大小的含义，以及 block 的数量是如何计算的，最后介绍 vLLM 是如何管理 block 的。</p> <h1 id="2-block-大小如何计算">2. Block 大小如何计算</h1> <p>block 的大小可以自定义，上面定义为 4，简单理解就是每个 block 最多存储 4 个 token 的 kv cache 数据。但是 block 设置为 4 的时候对应到 GPU 内存到底是多大呢？其实这很好计算，</p> <p>一个 block 占用内存大小（Byte）= token 数量 (block_size) ✖️ 一个 token 的 kv cache 占用 内存大小。</p> <p>所以，我们只需要计算出单个 token 的 kv cache 对应的大小即可。block 大小的计算方法由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/cache_engine.py</code>文件里<code class="language-plaintext highlighter-rouge">CacheEngine</code>类的<code class="language-plaintext highlighter-rouge">get_cache_block_size</code>函数实现，代码也很简单，简化后如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/worker/cache_engine.py
</span><span class="k">class</span> <span class="nc">CacheEngine</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_cache_block_size</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span>
        <span class="n">parallel_config</span><span class="p">:</span> <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_head_size</span><span class="p">()</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_kv_heads</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_layers</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="n">key_cache_block</span> <span class="o">=</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span>
        <span class="n">value_cache_block</span> <span class="o">=</span> <span class="n">key_cache_block</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_cache_block</span> <span class="o">+</span> <span class="n">value_cache_block</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_dtype</span> <span class="o">==</span> <span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="n">dtype</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">STR_DTYPE_TO_TORCH_DTYPE</span><span class="p">[</span><span class="n">cache_dtype</span><span class="p">]</span>
        <span class="n">dtype_size</span> <span class="o">=</span> <span class="nf">_get_dtype_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dtype_size</span> <span class="o">*</span> <span class="n">total</span>
</code></pre></div></div> <p>上面代码中首先拿到 <code class="language-plaintext highlighter-rouge">num_heads</code>和<code class="language-plaintext highlighter-rouge">head_size</code>两个变量的值， <code class="language-plaintext highlighter-rouge">num_heads * head_size</code>就表示单个 token 在单层多头注意力机制计算中所需要的参数量，不过这只是 key 或者 value cache 所占用的参数量。</p> <p>一个 block 占用的内存 = token 数量（block_size）✖️ 层数 (num_layers) ✖️ 单层 kv cache 占用内存 （2✖️num_heads✖️head_size）✖️ 数据类型大小（如果是 fp16，则每个数据占用 2 Bytes）</p> <p>举例来说，假设 block_size=4， num_layers=4, num_heads=8, heads_size=128，采用 fp16 存储数据，那么</p> <p>一个 block 占用内存大小 = 4 ✖️ 4 ✖️ 8 ✖️ 128 ✖️ 2 = 32,768 Bytes。</p> <p>总结，一个 block 所占用的内存大小就是 block_size 个 token kv cache 所占内存的总和。不同模型的 block 各不相同。</p> <h1 id="2-block-数量如何计算">2. Block 数量如何计算</h1> <p>block 数量计算由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/worker.py</code>文件中<code class="language-plaintext highlighter-rouge">Worker</code>类的<code class="language-plaintext highlighter-rouge">profile_num_available_blocks</code>函数实现，该函数很简单，简化代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Worker</span>
    <span class="nd">@torch.inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">profile_num_available_blocks</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">cpu_swap_space</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
		
		<span class="c1"># 这一行其实就是用模拟数据跑一下forward 来统计GPU 的使用情况
</span>        <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">profile_run</span><span class="p">()</span>

        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
        <span class="n">free_gpu_memory</span><span class="p">,</span> <span class="n">total_gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">mem_get_info</span><span class="p">()</span>
        <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">total_gpu_memory</span> <span class="o">-</span> <span class="n">free_gpu_memory</span>

        <span class="n">cache_block_size</span> <span class="o">=</span> <span class="n">CacheEngine</span><span class="p">.</span><span class="nf">get_cache_block_size</span><span class="p">(</span>
            <span class="n">block_size</span><span class="p">,</span> <span class="n">cache_dtype</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span>
            <span class="p">(</span><span class="n">total_gpu_memory</span> <span class="o">*</span> <span class="n">gpu_memory_utilization</span> <span class="o">-</span> <span class="n">peak_memory</span><span class="p">)</span> <span class="o">//</span>
            <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">cpu_swap_space</span> <span class="o">//</span> <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_cpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="n">lora_manager</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">remove_all_loras</span><span class="p">()</span>
        <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="n">num_cpu_blocks</span>
</code></pre></div></div> <p>整个函数的逻辑很清晰，简单理解就是先用模拟数据跑一次 forward 记录下 GPU 的使用情况，这样可以知道 peak memory，然后计算每个 block 需要用到的 memory，接着就可以计算出 block 数量了。具体而言：</p> <ul> <li>13 行：vllm 默认用 256 个句子来做 profile，每个句子长度为 128</li> <li>15 到 17 行：统计 GPU 内存使用情况，返回的是以字节（Byte）为单位的数值，后面也都是基于 Byte 为单位进行计算的</li> <li>19 行：计算每个 block 的大小，这个在前面已经介绍。</li> <li>20-23 行：计算可用的 GPU block 数量。<code class="language-plaintext highlighter-rouge">num_gpu_blocks = int( (total_gpu_memory * gpu_memory_utilization - peak_memory) // cache_block_size)</code>：gpu_memory_utilization: 默认值是 0.9，表示 GPU 内存利用率是 90%，这挺高的了。所以最终的可用 GPU block 数量等于剩余 GPU 内存大小除以每个 block 的大小</li> <li>24 行：计算可用的 CPU block 数量。 <code class="language-plaintext highlighter-rouge">num_cpu_blocks = int(cpu_swap_space // cache_block_size)</code>这里的cpu_swap_space 代表每个 GPU 对应的 CPU swap 空间大小，单位是（GB），默认是是 4。也就是说每个 GPU 对应的 CPU swap 空间大小是 4 GB。</li> </ul> <h1 id="3-block-如何管理">3. Block 如何管理？</h1> <h2 id="31-逻辑-block-定义和使用">3.1 逻辑 Block 定义和使用</h2> <p>逻辑 Block（<code class="language-plaintext highlighter-rouge">LogicalTokenBlock</code>）定义如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">LogicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">_BLANK_TOKEN_ID</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">get_num_empty_slots</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>

    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span>

    <span class="k">def</span> <span class="nf">append_tokens</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
        <span class="n">curr_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">:</span><span class="n">curr_idx</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_token_ids</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[:</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_last_token_id</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <ul> <li>block_number: int: 这个是 PhysicalTokenBlock 实例对象的索引，可以理解成是 flag，用于区分不同 block</li> <li>block_size: int： 表示一个 block 内存储多少个 token 的 kv cache 数据。</li> <li><code class="language-plaintext highlighter-rouge">__init__</code>函数中<code class="language-plaintext highlighter-rouge">self.token_ids</code>初始化是一个长度为 block_size 的全为 -1 的list。后续可以通过<code class="language-plaintext highlighter-rouge">append_tokens</code>将新的 token添加到这个 list 中去。</li> <li><code class="language-plaintext highlighter-rouge">self.num_tokens</code>会统计已使用的 token 数量，当<code class="language-plaintext highlighter-rouge">self.num_tokens==block_size</code>时则表示这个 block 已经被使用完了。</li> </ul> <p>逻辑 Block 的使用逻辑是根据需要实时实例化一个对象，如果当前的 <code class="language-plaintext highlighter-rouge">LogicalBlock</code>没有剩余空间了，就再实例化一个新的。</p> <p>在 vLLm 的使用场景是在<code class="language-plaintext highlighter-rouge">vllm/vllm/sequence.py</code>里的<code class="language-plaintext highlighter-rouge">Sequence</code>类中根据需要动态创建<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Sequence</code>类在之前介绍 vLLM 的文章 【<a href="https://zhuanlan.zhihu.com/p/681402162">大模型推理框架 vLLM 源码解析（一）</a>】中已经有详细介绍，这里你只需要知道这个类记录了每个输入句子整个推理过程（prefilling 和 decoding）的所有信息。</p> </blockquote> <p>我们结合代码来看会更好理解，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/sequence.py
</span><span class="k">class</span> <span class="nc">Sequence</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span>
		<span class="bp">...</span>
		<span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LogicalTokenBlock</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">self</span><span class="p">.</span><span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span>
		<span class="bp">...</span>

    <span class="k">def</span> <span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">cursor</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>

            <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">is_full</span><span class="p">():</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>
                <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">num_empty_slots</span> <span class="o">=</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
            <span class="n">last_block</span><span class="p">.</span><span class="nf">append_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="n">cursor</span><span class="p">:</span><span class="n">cursor</span> <span class="o">+</span>
                                               <span class="n">num_empty_slots</span><span class="p">])</span>
            <span class="n">cursor</span> <span class="o">+=</span> <span class="n">num_empty_slots</span>
			
    <span class="k">def</span> <span class="nf">_append_logical_block</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">block</span> <span class="o">=</span> <span class="nc">LogicalTokenBlock</span><span class="p">(</span>
            <span class="n">block_number</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">),</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">__init__</code>函数中会初始化<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>空数组，用来存<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。可以看到会先将 prompt 的所有 token 通过<code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>存入到 block 中</li> <li><code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>函数会遍历传入的 token_ids 数组中的每个 token id，将该 token 信息存入到 LogicalBlock 中。 <ul> <li>第 12 行：如果<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>为空，则会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个<code class="language-plaintext highlighter-rouge">LogicalBlock</code>，并存到<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>变量中去</li> <li>第 16 行：如果最新创建的<code class="language-plaintext highlighter-rouge">LogicalBlock</code>空间已经满了，则同样会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个新的<code class="language-plaintext highlighter-rouge">LogicalBlock</code></li> </ul> </li> </ul> <h2 id="32-物理block-定义和管理">3.2 物理Block 定义和管理</h2> <p>物理 Block (<code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>)的代码定义如下：</p> <ul> <li>device: Device: 是一个 enum.Enum 实例对象，要么是 CPU 要么是 GPU。</li> <li>self.ref_count 变量用来指示这个 block 被使用的次数，默认为 0，代表没有使用。可以大于等于1，表示这个 block 内 token的 cache 被重复利用，使用场景比如可以是 beam search，这样可以重复利用cache，减少内存开销。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">PhysicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="nf">return </span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">PhysicalTokenBlock(device=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">block_number=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">block_number</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">ref_count=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">ref_count</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>只是针对单个 block 的描述。vLLM 在<code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>文件下实现了<code class="language-plaintext highlighter-rouge">BlockAllocator</code>类用来初始化所有物理 block，并负责分配这些 block。</p> <p><code class="language-plaintext highlighter-rouge">BlockAllocator</code>这个类代码很简单，如下。主要作用有三个：</p> <ul> <li><code class="language-plaintext highlighter-rouge">__init__</code>: 初始化指定数量的物理层面 block，这个数量在前面一节已经介绍过如何计算。</li> <li><code class="language-plaintext highlighter-rouge">allocate</code>: 通过 list的 pop() 函数返回一个可用的 block，并将该 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>设置为 1</li> <li><code class="language-plaintext highlighter-rouge">free</code>：回收一个指定的 <code class="language-plaintext highlighter-rouge">PhysicalBlock</code>，但是回收的前提是这个 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>变量值为 0，表示这个 block 内的 token kv cache 数据不再需要了。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockAllocator</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
      <span class="n">self</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
      <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
      <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
      <span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span>

      <span class="c1"># Initialize the free blocks.
</span>      <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="nc">PhysicalTokenBlock</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                     <span class="n">block_number</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                                     <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PhysicalTokenBlock</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Out of memory! No free blocks are available.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">block</span>

  <span class="k">def</span> <span class="nf">free</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">PhysicalTokenBlock</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Double free! </span><span class="si">{</span><span class="n">block</span><span class="si">}</span><span class="s"> is already freed.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_num_free_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h2 id="33--block-管理和映射模块">3.3 Block 管理和映射模块</h2> <p>在介绍这个Block 管理模块之前，我们先了解 vLLM 中设置的用来判断句子是否能够被分配物理 Block 的三种状态，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">AllocStatus</span><span class="p">(</span><span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Result for BlockSpaceManager.can_allocate
    </span><span class="sh">"""</span>
    <span class="n">OK</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">LATER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">NEVER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
</code></pre></div></div> <p>三种状态的含义如下：</p> <ul> <li><code class="language-plaintext highlighter-rouge">OK</code>: seq_group 可以现在被分配。</li> <li><code class="language-plaintext highlighter-rouge">LATER</code>: seq_group 不能被分配。分配器的容量大于 seq_group 所需。</li> <li><code class="language-plaintext highlighter-rouge">NEVER</code>: seq_group 永远不能被分配。seq_group 太大，无法在 GPU 中分配。</li> </ul> <p><code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>下的<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>是一个高级内存管理器，它在内存密集型计算任务（尤其是在使用GPU和CPU进行大规模数据处理的情况下）中管理逻辑数据块和物理内存块之间的映射。</p> <p>接下来，我们结合代码介绍<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>一些重要的函数。</p> <ul> <li>初始化函数<code class="language-plaintext highlighter-rouge">__init__</code>: <ul> <li><code class="language-plaintext highlighter-rouge">watermark</code>: 一种阈值机制，用来决定何时停止在GPU上分配新的块，以避免内存不足</li> <li><code class="language-plaintext highlighter-rouge">watermark_blocks</code>: 计算出在达到内存不足前，还能在GPU上分配多少个块。</li> <li><code class="language-plaintext highlighter-rouge">sliding_window</code>: 可选参数，用来限制在任意给定时间内活跃的逻辑块的数量，有助于控制内存使用。</li> <li>创建了 cpu 和 gpu 两种 <code class="language-plaintext highlighter-rouge">BlockAllocator</code>,不过需要注意这里都是物理层面的 Block</li> <li>创建了一个字典 <code class="language-plaintext highlighter-rouge">block_tables</code>，用于存储每个 sequence id 和它所使用的物理块之间的映射。通过这个 sequence id ，我们就能找到对应的前面介绍的<code class="language-plaintext highlighter-rouge">Sequence</code>实例化对象，通过这个字典，就建立了逻辑 block 和物理 block 的映射关系。</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_gpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_cpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">watermark</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">=</span> <span class="n">num_gpu_blocks</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_cpu_blocks</span> <span class="o">=</span> <span class="n">num_cpu_blocks</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">sliding_window</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">sliding_window</span><span class="p">,</span>
                                                      <span class="n">block_size</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="n">sliding_window</span> <span class="o">//</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark</span> <span class="o">=</span> <span class="n">watermark</span>
        <span class="k">assert</span> <span class="n">watermark</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">watermark</span> <span class="o">*</span> <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">GPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_cpu_blocks</span><span class="p">)</span>
        <span class="c1"># Mapping: seq_id -&gt; BlockTable.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">BlockTable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">can_allocate</code> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="n">allocated</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">-=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="nf">get_num_blocks</span><span class="p">()</span>

      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">num_required_blocks</span><span class="p">,</span>
                                    <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span><span class="p">)</span>
      <span class="n">num_free_gpu_blocks</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">get_num_free_blocks</span><span class="p">()</span>

      <span class="c1"># Use watermark to avoid frequent cache eviction.
</span>      <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&lt;</span>
              <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">NEVER</span>
      <span class="k">if</span> <span class="n">num_free_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">OK</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">LATER</span>
</code></pre></div> </div> </li> </ul> <p><code class="language-plaintext highlighter-rouge">can_allocate</code>方法用于判断一个序列组（<code class="language-plaintext highlighter-rouge">seq_group</code>）是否能被成功分配所需的内存块。此方法首先计算该序列组基于当前任务的逻辑数据块所需的总物理内存块数量。接着，它会检查GPU分配器中的空闲内存块数量，以确认是否有足够的资源满足需求。</p> <p>方法中引入了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>概念，其主要目的是防止因频繁进行内存块的缓存淘汰而影响系统性能。在模型训练或数据处理的动态环境中，内存需求持续变化，如果因缺乏足够的空闲内存块而不得不频繁淘汰并重新分配内存块，将会造成性能损耗。这是因为被淘汰的内存块很可能很快再次需要使用，其重新分配过程会消耗额外的时间和资源。</p> <p>通过设置<code class="language-plaintext highlighter-rouge">watermark_blocks</code>阈值，当GPU上的空闲内存块数量低于此阈值时，系统将避免分配新的内存块，以留出缓冲区域，减少缓存淘汰的发生。只有当空闲内存块数量高于此阈值时，系统才会继续进行新的内存块分配。这种策略旨在平衡内存分配需求和系统性能，避免因频繁的内存操作而降低效率。</p> <p>如果根据当前的资源状态，确定序列组所需的内存块永远无法被满足，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.NEVER</code>，意味着该序列组在当前条件下无法被分配。如果当前不可分配但未来有可能，返回<code class="language-plaintext highlighter-rouge">AllocStatus.LATER</code>，表明序列组暂时无法分配，但随着系统状态的改变，可能在将来能够分配。如果有足够的空闲内存块满足分配需求，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.OK</code>，表示序列组可以立即被分配所需内存。</p> <p>这种方式确保了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>在满足内存分配需求的同时，有效避免了频繁的缓存淘汰问题，从而优化了整体的系统性能和资源利用效率。</p> <ul> <li><code class="language-plaintext highlighter-rouge">allocate</code> 代码有简化，但是不影响理解 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_prompt_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="n">block_table</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">logical_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_prompt_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">allocate</span><span class="p">()</span>
          <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">num_seqs</span><span class="p">()</span>
          <span class="n">block_table</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">):</span>
          <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">block_table</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
</code></pre></div> </div> <p><code class="language-plaintext highlighter-rouge">allocate</code> 方法用于为序列组分配内存块。它会遍历序列组中的每个序列，为每个序列分配足够的内存块，并将这些块添加到序列的块表中。同时，它会更新序列的块表，以便在后续的训练过程中可以正确地访问这些块。</p> </li> </ul> <p><code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>还有很多其它的函数，为了避免文章累赘，这里不做详细介绍。</p> <p>后面会继续写一篇 vLLM 的调度<code class="language-plaintext highlighter-rouge">Scheduler</code>模块的文章，对<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>更加详细地介绍。相信通过本篇文章，你应该能够对 vLLM 的 block 有一个清楚的了解了，如果还是不清楚，可以反复阅读直到清楚为止。</p> <h1 id="参考">参考</h1> <ul> <li>https://zhuanlan.zhihu.com/p/681018057</li> <li>https://zhuanlan.zhihu.com/p/656939628</li> <li>https://zhuanlan.zhihu.com/p/655561941</li> <li>https://zhuanlan.zhihu.com/p/658233994</li> <li>https://zhuanlan.zhihu.com/p/641999400</li> </ul> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="LLM"/><category term="Serving"/><category term="vLLM"/><category term="大模型推理"/><summary type="html"><![CDATA[1. Block 概览]]></summary></entry><entry><title type="html">大模型推理框架 vLLM 源码解析（一） - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-marsggbo/" rel="alternate" type="text/html" title="大模型推理框架 vLLM 源码解析（一） - marsggbo"/><published>2024-02-04T10:15:00+00:00</published><updated>2024-02-04T10:15:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. Quick Start 创建如下代码，命名为 run.py from vllm import LLM, SamplingParams prompts=[ &quot;Have you followed marsggbo in Zhihu?&quot;, &quot;你一键三连了吗？&quot; ] # 输入prompts sam]]></summary></entry><entry><title type="html">vllm 安装踩坑 (The NVIDIA driver on your system is too old) - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-the-nvidia-driver-on-your-system-is-too-old-marsggbo/" rel="alternate" type="text/html" title="vllm 安装踩坑 (The NVIDIA driver on your system is too old) - marsggbo"/><published>2024-01-15T12:35:00+00:00</published><updated>2024-01-15T12:35:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm--the-nvidia-driver-on-your-system-is-too-old---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-the-nvidia-driver-on-your-system-is-too-old-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[我的环境如下： nvidia-smi 显示 cuda 版本是 11.7 目前最新vllm 要求的 torch 版本是 2.1.2，该版本要求的 cuda 版本是 11.8，所以不匹配。执行安装会遇到如下错误 RuntimeError: The NVIDIA driver on your system]]></summary></entry></feed>