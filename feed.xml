<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://marsggbo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://marsggbo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-30T06:49:54+00:00</updated><id>https://marsggbo.github.io/feed.xml</id><title type="html">MARSGGBO’s World</title><subtitle>&quot;SSS: Study, Sleep, Slim&quot; </subtitle><entry><title type="html">大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/" rel="alternate" type="text/html" title="大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo"/><published>2024-03-23T12:48:00+00:00</published><updated>2024-03-23T12:48:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm-block----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. Block 概览 vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 bl]]></summary></entry><entry><title type="html">OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/" rel="alternate" type="text/html" title="OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo"/><published>2024-02-22T08:50:00+00:00</published><updated>2024-02-22T08:50:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/openai-soradiffusion---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[标题党一下，顺便蹭一下 OpenAI Sora大模型的热点，主要也是回顾一下扩散模型的原理。 1. 简单理解扩散模型 简单理解，扩散模型如下图所示可以分成两部分，一个是 forward，另一个是 reverse 过程： forward：这是加噪声的过程，表示为\(q(X_{0:T})\)，即在原图（]]></summary></entry><entry><title type="html">vLLM 源码解析（一）</title><link href="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/" rel="alternate" type="text/html" title="vLLM 源码解析（一）"/><published>2024-02-04T16:40:16+00:00</published><updated>2024-02-04T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/"><![CDATA[<h1 id="1-quick-start">1. Quick Start</h1> <p>创建如下代码，命名为 <code class="language-plaintext highlighter-rouge">run.py</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
	<span class="sh">"</span><span class="s">Have you followed marsggbo in Zhihu?</span><span class="sh">"</span><span class="p">,</span>
	<span class="sh">"</span><span class="s">你一键三连了吗？</span><span class="sh">"</span>
<span class="p">]</span> <span class="c1"># 输入prompts
</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># 采样策略
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">facebook/opt-125m</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 初始化 LLM
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span> <span class="c1"># 完成推理
</span><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
	<span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>执行命令：<code class="language-plaintext highlighter-rouge">python run.py</code>。该脚本会自动将模型以张量并行的方式在两个 GPU 上进行推理计算。</p> <p>整个推理过程大大致流程如下图所示，即</p> <ul> <li>1 给定一定数量的 prompts（字符串数组）</li> <li> <ol> <li>vllm 会使用 Scheduler 模块自动对需要推理句子进行调度</li> </ol> </li> <li> <ol> <li>根据调度的结果，使用 tokenizer 将字符串转换成 prompt id，然后喂给 model 进行计算得到 logits 预测结果</li> </ol> </li> <li> <ol> <li>根据 logits 预测结果和提前设置好的采样策略对结果进行采样得到新的 token id</li> </ol> </li> <li> <ol> <li>将采样结果保存到 output</li> </ol> </li> </ul> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707030203438.png" alt="inferencce pipeline"/></p> <h1 id="2-整体核心模块">2. 整体核心模块</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707037549078.png" alt="vllm 核心模块结构"/></p> <p>上图给出了 vLLM 核心模块之间的结构关系。接下来我们从简单的模块（即输入、采样和输出）开始介绍，最后详细介绍 LLM 模块。</p> <h1 id="3-sequence">3. Sequence</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707031824422.png" alt="句子模块"/></p> <p>如上图我们可以看到 vLLM 为输入的句子设计了很多子模块，这些模块的用处各不相同，但是有彼此之间有关系，下面分别详细介绍一下。</p> <h1 id="31-sequencestatus">3.1 SequenceStatus</h1> <p>首先看到 <code class="language-plaintext highlighter-rouge">SequenceStatus</code>，其源代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceStatus</span><span class="p">(</span><span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Status of a sequence.</span><span class="sh">"""</span>
    <span class="n">WAITING</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 等待中，句子还没开始推理，或者推理还未结束
</span>    <span class="n">RUNNING</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 运行中
</span>    <span class="n">SWAPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已交换
</span>    <span class="n">FINISHED_STOPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已停止
</span>    <span class="n">FINISHED_LENGTH_CAPPED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已长度限制
</span>    <span class="n">FINISHED_ABORTED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已中止
</span>    <span class="n">FINISHED_IGNORED</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span> <span class="c1"># 已忽略
</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_finished</span><span class="p">(</span><span class="n">status</span><span class="p">:</span> <span class="sh">"</span><span class="s">SequenceStatus</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># 判断状态是否为已停止、已长度限制、已中止或已忽略
</span>        <span class="k">return</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_STOPPED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_LENGTH_CAPPED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_ABORTED</span><span class="p">,</span>
            <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">FINISHED_IGNORED</span><span class="p">,</span>
        <span class="p">]</span>
</code></pre></div></div> <h2 id="32-sequencedata">3.2 SequenceData</h2> <p><code class="language-plaintext highlighter-rouge">SequenceData</code> 用于存储与序列相关的数据。这个类有三个属性：<code class="language-plaintext highlighter-rouge">prompt_token_ids</code>（提示词的标记ID）、<code class="language-plaintext highlighter-rouge">output_token_ids</code>（生成文本的标记ID）和<code class="language-plaintext highlighter-rouge">cumulative_logprob</code>（累计对数概率）。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceData</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">prompt_token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cumulative_logprob</span> <span class="o">=</span> <span class="mf">0.0</span>
</code></pre></div></div> <h2 id="33-sequence">3.3 Sequence</h2> <p><code class="language-plaintext highlighter-rouge">Sequence</code> 用于存储序列的数据、状态和块信息,且每个序列有唯一标识，即<code class="language-plaintext highlighter-rouge">seq_id</code>。注意看下面的代码：</p> <ul> <li><strong>数据</strong>其实是通过上面的 <code class="language-plaintext highlighter-rouge">SequenceData</code> 保存的</li> <li>默认初始化状态，所有句子序列的<strong>状态</strong>都是 <code class="language-plaintext highlighter-rouge">SequenceStatus.WAITING</code></li> <li>所谓<strong>块信息</strong>，其实就是 vLLM 会在初始化阶段预留出一定数量的CPU 和 GPU 内存，一般是以 token 为单位的，例如在初始化的时候会使用值全为 0，大小为 (256, 128)的 prompt_ids做 warm up。每个序列会按照实际大小申请 block 来记录内存使用情况，即序列 token 数越多，属性<code class="language-plaintext highlighter-rouge">logical_token_blocks</code>包含的 block 个数也就越多。</li> <li> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sequence</span><span class="p">:</span>
<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">seq_id</span> <span class="o">=</span> <span class="n">seq_id</span>
    <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span>
    <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

    <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="nc">SequenceData</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="c1"># 数据
</span>
    <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LogicalTokenBlock</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Initialize the logical token blocks with the prompt token ids.
</span>    <span class="n">self</span><span class="p">.</span><span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="c1"># 块信息
</span>    <span class="n">self</span><span class="p">.</span><span class="n">status</span> <span class="o">=</span> <span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span> <span class="c1"># 状态
</span>    <span class="bp">...</span>
</code></pre></div> </div> </li> </ul> <h2 id="33-sequencegroup">3.3 SequenceGroup</h2> <p><code class="language-plaintext highlighter-rouge">Sequence</code>只是单个序列的表示方式,<code class="language-plaintext highlighter-rouge">seq_id</code>是它的唯一标识。<code class="language-plaintext highlighter-rouge">SequenceGroup</code>则是为了表示多个序列，<code class="language-plaintext highlighter-rouge">request_id</code>是它的唯一标识，表示是第几个请求。</p> <p>具体而言，可以看到<code class="language-plaintext highlighter-rouge">__init__</code>函数有个参数是 <code class="language-plaintext highlighter-rouge">seqs: List[Sequence]</code>，它表示由一个或多个 Sequence 组成的列表，然后会通过<code class="language-plaintext highlighter-rouge">self.seqs_dict = {seq.seq_id: seq for seq in seqs}</code>转化成字典方便管理，这个字典的 key 是每个 Sequence 的唯一标识<code class="language-plaintext highlighter-rouge">seq_id</code>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceGroup</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Sequence</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">arrival_time</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Prefix</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seqs_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span><span class="p">:</span> <span class="n">seq</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seqs</span><span class="p">}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">sampling_params</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arrival_time</span> <span class="o">=</span> <span class="n">arrival_time</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>下面是 vLLm 中 LLMEngine 使用 Sequence 和 SequenceGroup 的场景示例：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LLMEngine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">add_request</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">arrival_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prefix_pos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode_request</span><span class="p">(</span>
            <span class="n">request_id</span><span class="o">=</span><span class="n">request_id</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">prompt_token_ids</span><span class="p">,</span>
            <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_request</span><span class="p">)</span> <span class="c1"># 将字符串序列转换成 id
</span>
        <span class="c1"># Create the sequences.
</span>        <span class="n">block_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cache_config</span><span class="p">.</span><span class="n">block_size</span>
        <span class="n">seq_id</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">seq_counter</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="nc">Sequence</span><span class="p">(</span><span class="n">seq_id</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                       <span class="n">lora_request</span><span class="p">)</span>

        <span class="c1"># Create the sequence group.
</span>        <span class="n">seq_group</span> <span class="o">=</span> <span class="nc">SequenceGroup</span><span class="p">(</span><span class="n">request_id</span><span class="p">,</span> <span class="p">[</span><span class="n">seq</span><span class="p">],</span> <span class="n">sampling_params</span><span class="p">,</span>
                                  <span class="n">arrival_time</span><span class="p">)</span>

        <span class="c1"># Add the sequence group to the scheduler.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">add_seq_group</span><span class="p">(</span><span class="n">seq_group</span><span class="p">)</span>
</code></pre></div></div> <p>可以看到<code class="language-plaintext highlighter-rouge">SequenceGroup</code>的<code class="language-plaintext highlighter-rouge">seqs</code>参数在最初阶段其实只是单个序列 ，即<code class="language-plaintext highlighter-rouge">[seq]</code>。但是我们知道其实一个 prompt 可以有多个输出结果，所以<code class="language-plaintext highlighter-rouge">SequenceGroup</code>的目的是管理一个输入 prompt的多个生成序列信息。如果我们设置<code class="language-plaintext highlighter-rouge">SamplingParams.n=2</code>（第 4 节会介绍），那么在推理过程中，<code class="language-plaintext highlighter-rouge">SequenceGroup</code>会新增一个 Sequence，这个新增的 Sequence 的 seq_id 和原来的那个 Sequence 不一样，具体的代码细节会在下一篇文章中介绍。</p> <h2 id="35-sequencegroupmetadata">3.5 SequenceGroupMetadata</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceGroupMetadata</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">is_prompt</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">seq_data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceData</span><span class="p">],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">is_prompt</span> <span class="o">=</span> <span class="n">is_prompt</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seq_data</span> <span class="o">=</span> <span class="n">seq_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">sampling_params</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span> <span class="o">=</span> <span class="n">block_tables</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>SequenceGroupMetadata 记录了一些元信息，下面代码展示了 Scheduler 模块是如何生成这些信息的：</p> <ul> <li><code class="language-plaintext highlighter-rouge">request_id</code> 就是 SequenceGroup的 request_id</li> <li><code class="language-plaintext highlighter-rouge">seq_data</code> 是一个字典，key 是每个 Sequence的 seq_id，value 则是对应的 data （即 SequenceData）</li> <li><code class="language-plaintext highlighter-rouge">block_tables</code>也是一个字典，key 也是每个 Sequence的 seq_id，value 这是对应 Sequence 申请的 block</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">schedule</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">],</span> <span class="n">SchedulerOutputs</span><span class="p">]:</span>
        <span class="n">scheduler_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_schedule</span><span class="p">()</span>

        <span class="c1"># Create input data structures.
</span>        <span class="n">seq_group_metadata_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq_group</span> <span class="ow">in</span> <span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">scheduled_seq_groups</span><span class="p">:</span>
            <span class="n">seq_data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceData</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">RUNNING</span><span class="p">):</span>
                <span class="n">seq_id</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span>
                <span class="n">seq_data</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">data</span> <span class="c1"># 单个 SequenceData
</span>                <span class="n">block_tables</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">block_manager</span><span class="p">.</span><span class="nf">get_block_table</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="c1"># 对应Sequence的block信息
</span>
            <span class="n">seq_group_metadata</span> <span class="o">=</span> <span class="nc">SequenceGroupMetadata</span><span class="p">(</span>
                <span class="n">request_id</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">request_id</span><span class="p">,</span>
                <span class="n">is_prompt</span><span class="o">=</span><span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">prompt_run</span><span class="p">,</span>
                <span class="n">seq_data</span><span class="o">=</span><span class="n">seq_data</span><span class="p">,</span>
                <span class="n">sampling_params</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">sampling_params</span><span class="p">,</span>
                <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
                <span class="n">lora_request</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">lora_request</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">seq_group_metadata_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">seq_group_metadata</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">seq_group_metadata_list</span><span class="p">,</span> <span class="n">scheduler_outputs</span>
</code></pre></div></div> <h2 id="36-sequenceoutput-和-sequencegroupoutput">3.6 SequenceOutput 和 SequenceGroupOutput</h2> <p>SequenceOutput 和 SequenceGroupOutput的关系就类似 Sequence 和 SequenceGroup。SequenceOutput其实就是记录了上一个 输入 token id 以及对应输出的 token id。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">parent_seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_token</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">parent_seq_id</span> <span class="o">=</span> <span class="n">parent_seq_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_token</span> <span class="o">=</span> <span class="n">output_token</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logprobs</span> <span class="o">=</span> <span class="n">logprobs</span>

<span class="k">class</span> <span class="nc">SequenceGroupOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">samples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceOutput</span><span class="p">],</span>
        <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PromptLogprobs</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_logprobs</span> <span class="o">=</span> <span class="n">prompt_logprobs</span>
</code></pre></div></div> <h1 id="4-samplingparams">4. SamplingParams</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707037767316.png" alt="SamplingParams"/></p> <p>SamplingParams 包含以下参数：</p> <ul> <li><code class="language-plaintext highlighter-rouge">n</code>：要生成的序列的数量，默认为 1。</li> <li><code class="language-plaintext highlighter-rouge">best_of</code>：从多少个序列中选择最佳序列，需要大于 n，默认等于 n。</li> <li><code class="language-plaintext highlighter-rouge">temperature</code>：用于控制生成结果的随机性，较低的温度会使生成结果更确定性，较高的温度会使生成结果更随机。</li> <li><code class="language-plaintext highlighter-rouge">top_p</code>：用于过滤掉生成词汇表中概率低于给定阈值的词汇，控制随机性。</li> <li><code class="language-plaintext highlighter-rouge">top_k</code>：选择前 k 个候选 token，控制多样性。</li> <li><code class="language-plaintext highlighter-rouge">presence_penalty</code>：用于控制生成结果中特定词汇的出现频率。</li> <li><code class="language-plaintext highlighter-rouge">frequency_penalty</code>：用于控制生成结果中词汇的频率分布。</li> <li><code class="language-plaintext highlighter-rouge">repetition_penalty</code>：用于控制生成结果中的词汇重复程度。</li> <li><code class="language-plaintext highlighter-rouge">use_beam_search</code>：是否使用束搜索来生成序列。</li> <li><code class="language-plaintext highlighter-rouge">length_penalty</code>：用于控制生成结果的长度分布。</li> <li><code class="language-plaintext highlighter-rouge">early_stopping</code>：是否在生成过程中提前停止。</li> <li><code class="language-plaintext highlighter-rouge">stop</code>：要停止生成的词汇列表。</li> <li><code class="language-plaintext highlighter-rouge">stop_token_ids</code>：要停止生成的词汇的ID列表。</li> <li><code class="language-plaintext highlighter-rouge">include_stop_str_in_output</code>：是否在输出结果中包含停止字符串。</li> <li><code class="language-plaintext highlighter-rouge">ignore_eos</code>：在生成过程中是否忽略结束符号。</li> <li><code class="language-plaintext highlighter-rouge">max_tokens</code>：生成序列的最大长度。</li> <li><code class="language-plaintext highlighter-rouge">logprobs</code>：用于记录生成过程的概率信息。</li> <li><code class="language-plaintext highlighter-rouge">prompt_logprobs</code>：用于记录生成过程的概率信息，用于特定提示。</li> <li><code class="language-plaintext highlighter-rouge">skip_special_tokens</code>：是否跳过特殊符号。</li> <li><code class="language-plaintext highlighter-rouge">spaces_between_special_tokens</code>：是否在特殊符号之间添加空格。</li> </ul> <p>这些参数的设置通常取决于具体需求和模型性能。以下是一些常见的设置指导方法：</p> <ul> <li><code class="language-plaintext highlighter-rouge">temperature</code>：较低的温度（如0.2）会产生更确定性的结果，而较高的温度（如0.8）会产生更随机的结果。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">presence_penalty、frequency_penalty 和 repetition_penalty</code>：这些参数可以用于控制生成结果中的词汇分布和重复程度。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">use_beam_search</code>：束搜索通常用于生成更高质量的结果，但可能会降低生成速度。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">length_penalty</code>：这个参数可以用于控制生成结果的长度。较高的值会产生更长的结果，而较低的值会产生更短的结果。您可以根据您的需求进行调整。</li> <li><code class="language-plaintext highlighter-rouge">early_stopping</code>：如果您不希望生成过长的结果，可以设置此参数为True。</li> <li><code class="language-plaintext highlighter-rouge">stop 和 stop_token_ids</code>：您可以使用这些参数来指定生成结果的结束条件。</li> </ul> <h1 id="5-output-模块">5. Output 模块</h1> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_4_1707040962845.png" alt="Output模块"/></p> <p>Output 主要用于表示语言模型（LLM）的生成结果，包含如下两个模块：</p> <ul> <li><code class="language-plaintext highlighter-rouge">CompletionOutput</code></li> <li><code class="language-plaintext highlighter-rouge">RequestOutput</code></li> </ul> <p>通过上面的介绍我们知道一个 request 可能包含多个序列，<code class="language-plaintext highlighter-rouge">CompletionOutput</code> 用来表示一个 request 中某个序列的完整输出的数据，其中下面的<code class="language-plaintext highlighter-rouge">index</code>就表示该序列在 request 中的索引位置</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CompletionOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># 输出结果在请求中的索引
</span>        <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="c1"># 生成的文本
</span>        <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="c1"># 生成的文本对应的 token ID 列表
</span>        <span class="n">cumulative_logprob</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleLogprobs</span><span class="p">],</span>
        <span class="n">finish_reason</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># 序列完成的原因（SequenceStatus）
</span>        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
        <span class="n">self</span><span class="p">.</span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">finish_reason</span> <span class="o">=</span> <span class="n">finish_reason</span>
		<span class="bp">...</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">RequestOutput</code>则表示 request 所有序列的输出结果，有它的初始化函数可以看到它记录了对应的 <code class="language-plaintext highlighter-rouge">request_id</code>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RequestOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PromptLogprobs</span><span class="p">],</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompletionOutput</span><span class="p">],</span>
        <span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">request_id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">prompt_token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">self</span><span class="p">.</span><span class="n">finished</span> <span class="o">=</span> <span class="n">finished</span>
		<span class="bp">...</span>
</code></pre></div></div> <p>我们看看RequestOutput的from_seq_group就能很好理解<code class="language-plaintext highlighter-rouge">CompletionOutput</code>和 <code class="language-plaintext highlighter-rouge">RequestOutput</code>是如何使用的了。为方便理解，代码有删减，但是不影响最终结果：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RequestOutput</span><span class="p">:</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_seq_group</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">RequestOutput</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># 1. Get the top-n sequences.
</span>        <span class="n">n</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">sampling_params</span><span class="p">.</span><span class="n">n</span> <span class="c1"># 每个序列返回的生成序列数量
</span>        <span class="n">seqs</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">()</span>
		<span class="c1"># 根据累积 logprob 值来选择出前 n 个生成序列
</span>		<span class="n">sorting_key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">seq</span><span class="p">:</span> <span class="n">seq</span><span class="p">.</span><span class="nf">get_cumulative_logprob</span><span class="p">()</span>
        <span class="n">sorted_seqs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">sorting_key</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">top_n_seqs</span> <span class="o">=</span> <span class="n">sorted_seqs</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>

        <span class="c1"># 2. Create the outputs.
</span>        <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompletionOutput</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">top_n_seqs</span><span class="p">:</span>
            <span class="n">logprobs</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="n">output_logprobs</span>
            <span class="n">finshed_reason</span> <span class="o">=</span> <span class="n">SequenceStatus</span><span class="p">.</span><span class="nf">get_finished_reason</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">status</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nc">CompletionOutput</span><span class="p">(</span><span class="n">seqs</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="n">output_text</span><span class="p">,</span>
                                      <span class="n">seq</span><span class="p">.</span><span class="nf">get_output_token_ids</span><span class="p">(),</span>
                                      <span class="n">seq</span><span class="p">.</span><span class="nf">get_cumulative_logprob</span><span class="p">(),</span> <span class="n">logprobs</span><span class="p">,</span>
                                      <span class="n">finshed_reason</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Every sequence in the sequence group should have the same prompt.
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt_token_ids</span>
        <span class="n">prompt_logprobs</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prompt_logprobs</span>
        <span class="n">finished</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">is_finished</span><span class="p">()</span>
        <span class="k">return</span> <span class="nf">cls</span><span class="p">(</span><span class="n">seq_group</span><span class="p">.</span><span class="n">request_id</span><span class="p">,</span>
                   <span class="n">prompt</span><span class="p">,</span>
                   <span class="n">prompt_token_ids</span><span class="p">,</span>
                   <span class="n">prompt_logprobs</span><span class="p">,</span>
                   <span class="n">outputs</span><span class="p">,</span>
                   <span class="n">finished</span><span class="p">,</span>
                   <span class="n">lora_request</span><span class="o">=</span><span class="n">seq_group</span><span class="p">.</span><span class="n">lora_request</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">RequestOutput</code>是通过对传入的<code class="language-plaintext highlighter-rouge">seq_group: SequenceGroup</code>进行解析后得到的。解析过程主要有两个阶段：</p> <ul> <li> <ol> <li>Get the top-n sequences：这一阶段就是对生成序列按照 cumulative_logprob 进行排序，最后选择出top-n 序列。</li> </ol> </li> <li> <ol> <li>Create the outputs：将所有top-n生成序列分别转换成 <code class="language-plaintext highlighter-rouge">CompletionOutput</code>列表，并作为<code class="language-plaintext highlighter-rouge">RequestOutput</code>的初始化参数。</li> </ol> </li> </ul> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="LLM"/><category term="Serving"/><category term="vLLM"/><category term="大模型推理"/><summary type="html"><![CDATA[1. Quick Start]]></summary></entry><entry><title type="html">vLLM 源码解析（二）</title><link href="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-Block-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86/" rel="alternate" type="text/html" title="vLLM 源码解析（二）"/><published>2024-02-04T16:40:16+00:00</published><updated>2024-02-04T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%20Block%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-Block-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86/"><![CDATA[<h1 id="1-block-概览">1. Block 概览</h1> <p>vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 block 的关系。</p> <p>假设每个 block 可以用来存 4 个 token 的kv cache数据。一个句子的 token在逻辑层面是紧邻的，每次 decoding 生成新的 token 就往空闲的 block 里放。但是对应到物理层面的 block，一个句子的 token 可能分布在并不相邻的 block内，不过没关系，vLLM 会为每个句子的每个 token记录逻辑和物理block 的映射关系，方便查找和读取。</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_3_23_1711187884361.png" alt="vLLM Block"/></p> <p>接下来我们详细介绍 block 大小的含义，以及 block 的数量是如何计算的，最后介绍 vLLM 是如何管理 block 的。</p> <h1 id="2-block-大小如何计算">2. Block 大小如何计算</h1> <p>block 的大小可以自定义，上面定义为 4，简单理解就是每个 block 最多存储 4 个 token 的 kv cache 数据。但是 block 设置为 4 的时候对应到 GPU 内存到底是多大呢？其实这很好计算，</p> <p>一个 block 占用内存大小（Byte）= token 数量 (block_size) ✖️ 一个 token 的 kv cache 占用 内存大小。</p> <p>所以，我们只需要计算出单个 token 的 kv cache 对应的大小即可。block 大小的计算方法由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/cache_engine.py</code>文件里<code class="language-plaintext highlighter-rouge">CacheEngine</code>类的<code class="language-plaintext highlighter-rouge">get_cache_block_size</code>函数实现，代码也很简单，简化后如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/worker/cache_engine.py
</span><span class="k">class</span> <span class="nc">CacheEngine</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_cache_block_size</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span>
        <span class="n">parallel_config</span><span class="p">:</span> <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_head_size</span><span class="p">()</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_kv_heads</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="nf">get_num_layers</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="n">key_cache_block</span> <span class="o">=</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span>
        <span class="n">value_cache_block</span> <span class="o">=</span> <span class="n">key_cache_block</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_cache_block</span> <span class="o">+</span> <span class="n">value_cache_block</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_dtype</span> <span class="o">==</span> <span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="n">dtype</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">STR_DTYPE_TO_TORCH_DTYPE</span><span class="p">[</span><span class="n">cache_dtype</span><span class="p">]</span>
        <span class="n">dtype_size</span> <span class="o">=</span> <span class="nf">_get_dtype_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dtype_size</span> <span class="o">*</span> <span class="n">total</span>
</code></pre></div></div> <p>上面代码中首先拿到 <code class="language-plaintext highlighter-rouge">num_heads</code>和<code class="language-plaintext highlighter-rouge">head_size</code>两个变量的值， <code class="language-plaintext highlighter-rouge">num_heads * head_size</code>就表示单个 token 在单层多头注意力机制计算中所需要的参数量，不过这只是 key 或者 value cache 所占用的参数量。</p> <p>一个 block 占用的内存 = token 数量（block_size）✖️ 层数 (num_layers) ✖️ 单层 kv cache 占用内存 （2✖️num_heads✖️head_size）✖️ 数据类型大小（如果是 fp16，则每个数据占用 2 Bytes）</p> <p>举例来说，假设 block_size=4， num_layers=4, num_heads=8, heads_size=128，采用 fp16 存储数据，那么</p> <p>一个 block 占用内存大小 = 4 ✖️ 4 ✖️ 8 ✖️ 128 ✖️ 2 = 32,768 Bytes。</p> <p>总结，一个 block 所占用的内存大小就是 block_size 个 token kv cache 所占内存的总和。不同模型的 block 各不相同。</p> <h1 id="2-block-数量如何计算">2. Block 数量如何计算</h1> <p>block 数量计算由<code class="language-plaintext highlighter-rouge">vllm/vllm/worker/worker.py</code>文件中<code class="language-plaintext highlighter-rouge">Worker</code>类的<code class="language-plaintext highlighter-rouge">profile_num_available_blocks</code>函数实现，该函数很简单，简化代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Worker</span>
    <span class="nd">@torch.inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">profile_num_available_blocks</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">cpu_swap_space</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
		
		<span class="c1"># 这一行其实就是用模拟数据跑一下forward 来统计GPU 的使用情况
</span>        <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">profile_run</span><span class="p">()</span>

        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
        <span class="n">free_gpu_memory</span><span class="p">,</span> <span class="n">total_gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">mem_get_info</span><span class="p">()</span>
        <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">total_gpu_memory</span> <span class="o">-</span> <span class="n">free_gpu_memory</span>

        <span class="n">cache_block_size</span> <span class="o">=</span> <span class="n">CacheEngine</span><span class="p">.</span><span class="nf">get_cache_block_size</span><span class="p">(</span>
            <span class="n">block_size</span><span class="p">,</span> <span class="n">cache_dtype</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span>
            <span class="p">(</span><span class="n">total_gpu_memory</span> <span class="o">*</span> <span class="n">gpu_memory_utilization</span> <span class="o">-</span> <span class="n">peak_memory</span><span class="p">)</span> <span class="o">//</span>
            <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">cpu_swap_space</span> <span class="o">//</span> <span class="n">cache_block_size</span><span class="p">)</span>
        <span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">num_cpu_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="n">lora_manager</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="nf">remove_all_loras</span><span class="p">()</span>
        <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">num_gpu_blocks</span><span class="p">,</span> <span class="n">num_cpu_blocks</span>
</code></pre></div></div> <p>整个函数的逻辑很清晰，简单理解就是先用模拟数据跑一次 forward 记录下 GPU 的使用情况，这样可以知道 peak memory，然后计算每个 block 需要用到的 memory，接着就可以计算出 block 数量了。具体而言：</p> <ul> <li>13 行：vllm 默认用 256 个句子来做 profile，每个句子长度为 128</li> <li>15 到 17 行：统计 GPU 内存使用情况，返回的是以字节（Byte）为单位的数值，后面也都是基于 Byte 为单位进行计算的</li> <li>19 行：计算每个 block 的大小，这个在前面已经介绍。</li> <li>20-23 行：计算可用的 GPU block 数量。<code class="language-plaintext highlighter-rouge">num_gpu_blocks = int( (total_gpu_memory * gpu_memory_utilization - peak_memory) // cache_block_size)</code>：gpu_memory_utilization: 默认值是 0.9，表示 GPU 内存利用率是 90%，这挺高的了。所以最终的可用 GPU block 数量等于剩余 GPU 内存大小除以每个 block 的大小</li> <li>24 行：计算可用的 CPU block 数量。 <code class="language-plaintext highlighter-rouge">num_cpu_blocks = int(cpu_swap_space // cache_block_size)</code>这里的cpu_swap_space 代表每个 GPU 对应的 CPU swap 空间大小，单位是（GB），默认是是 4。也就是说每个 GPU 对应的 CPU swap 空间大小是 4 GB。</li> </ul> <h1 id="3-block-如何管理">3. Block 如何管理？</h1> <h2 id="31-逻辑-block-定义和使用">3.1 逻辑 Block 定义和使用</h2> <p>逻辑 Block（<code class="language-plaintext highlighter-rouge">LogicalTokenBlock</code>）定义如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">LogicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">_BLANK_TOKEN_ID</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">get_num_empty_slots</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>

    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">block_size</span>

    <span class="k">def</span> <span class="nf">append_tokens</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
        <span class="n">curr_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">:</span><span class="n">curr_idx</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token_ids</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_token_ids</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[:</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_last_token_id</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">token_ids</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <ul> <li>block_number: int: 这个是 PhysicalTokenBlock 实例对象的索引，可以理解成是 flag，用于区分不同 block</li> <li>block_size: int： 表示一个 block 内存储多少个 token 的 kv cache 数据。</li> <li><code class="language-plaintext highlighter-rouge">__init__</code>函数中<code class="language-plaintext highlighter-rouge">self.token_ids</code>初始化是一个长度为 block_size 的全为 -1 的list。后续可以通过<code class="language-plaintext highlighter-rouge">append_tokens</code>将新的 token添加到这个 list 中去。</li> <li><code class="language-plaintext highlighter-rouge">self.num_tokens</code>会统计已使用的 token 数量，当<code class="language-plaintext highlighter-rouge">self.num_tokens==block_size</code>时则表示这个 block 已经被使用完了。</li> </ul> <p>逻辑 Block 的使用逻辑是根据需要实时实例化一个对象，如果当前的 <code class="language-plaintext highlighter-rouge">LogicalBlock</code>没有剩余空间了，就再实例化一个新的。</p> <p>在 vLLm 的使用场景是在<code class="language-plaintext highlighter-rouge">vllm/vllm/sequence.py</code>里的<code class="language-plaintext highlighter-rouge">Sequence</code>类中根据需要动态创建<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Sequence</code>类在之前介绍 vLLM 的文章 【<a href="https://zhuanlan.zhihu.com/p/681402162">大模型推理框架 vLLM 源码解析（一）</a>】中已经有详细介绍，这里你只需要知道这个类记录了每个输入句子整个推理过程（prefilling 和 decoding）的所有信息。</p> </blockquote> <p>我们结合代码来看会更好理解，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/sequence.py
</span><span class="k">class</span> <span class="nc">Sequence</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span>
		<span class="bp">...</span>
		<span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LogicalTokenBlock</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="n">self</span><span class="p">.</span><span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span>
		<span class="bp">...</span>

    <span class="k">def</span> <span class="nf">_append_tokens_to_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">cursor</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>

            <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">is_full</span><span class="p">():</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">_append_logical_block</span><span class="p">()</span>
                <span class="n">last_block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">num_empty_slots</span> <span class="o">=</span> <span class="n">last_block</span><span class="p">.</span><span class="nf">get_num_empty_slots</span><span class="p">()</span>
            <span class="n">last_block</span><span class="p">.</span><span class="nf">append_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="n">cursor</span><span class="p">:</span><span class="n">cursor</span> <span class="o">+</span>
                                               <span class="n">num_empty_slots</span><span class="p">])</span>
            <span class="n">cursor</span> <span class="o">+=</span> <span class="n">num_empty_slots</span>
			
    <span class="k">def</span> <span class="nf">_append_logical_block</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">block</span> <span class="o">=</span> <span class="nc">LogicalTokenBlock</span><span class="p">(</span>
            <span class="n">block_number</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">),</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">__init__</code>函数中会初始化<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>空数组，用来存<code class="language-plaintext highlighter-rouge">LogicalBlock</code>。可以看到会先将 prompt 的所有 token 通过<code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>存入到 block 中</li> <li><code class="language-plaintext highlighter-rouge">_append_tokens_to_blocks</code>函数会遍历传入的 token_ids 数组中的每个 token id，将该 token 信息存入到 LogicalBlock 中。 <ul> <li>第 12 行：如果<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>为空，则会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个<code class="language-plaintext highlighter-rouge">LogicalBlock</code>，并存到<code class="language-plaintext highlighter-rouge">self.logical_token_blocks</code>变量中去</li> <li>第 16 行：如果最新创建的<code class="language-plaintext highlighter-rouge">LogicalBlock</code>空间已经满了，则同样会动态调用<code class="language-plaintext highlighter-rouge">_append_logical_block</code>来创建一个新的<code class="language-plaintext highlighter-rouge">LogicalBlock</code></li> </ul> </li> </ul> <h2 id="32-物理block-定义和管理">3.2 物理Block 定义和管理</h2> <p>物理 Block (<code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>)的代码定义如下：</p> <ul> <li>device: Device: 是一个 enum.Enum 实例对象，要么是 CPU 要么是 GPU。</li> <li>self.ref_count 变量用来指示这个 block 被使用的次数，默认为 0，代表没有使用。可以大于等于1，表示这个 block 内 token的 cache 被重复利用，使用场景比如可以是 beam search，这样可以重复利用cache，减少内存开销。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/block.py
</span><span class="k">class</span> <span class="nc">PhysicalTokenBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
        <span class="n">block_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_number</span> <span class="o">=</span> <span class="n">block_number</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="nf">return </span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">PhysicalTokenBlock(device=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">block_number=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">block_number</span><span class="si">}</span><span class="s">, </span><span class="sh">'</span>
                <span class="sa">f</span><span class="sh">'</span><span class="s">ref_count=</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">ref_count</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">PhysicalTokenBlock</code>只是针对单个 block 的描述。vLLM 在<code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>文件下实现了<code class="language-plaintext highlighter-rouge">BlockAllocator</code>类用来初始化所有物理 block，并负责分配这些 block。</p> <p><code class="language-plaintext highlighter-rouge">BlockAllocator</code>这个类代码很简单，如下。主要作用有三个：</p> <ul> <li><code class="language-plaintext highlighter-rouge">__init__</code>: 初始化指定数量的物理层面 block，这个数量在前面一节已经介绍过如何计算。</li> <li><code class="language-plaintext highlighter-rouge">allocate</code>: 通过 list的 pop() 函数返回一个可用的 block，并将该 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>设置为 1</li> <li><code class="language-plaintext highlighter-rouge">free</code>：回收一个指定的 <code class="language-plaintext highlighter-rouge">PhysicalBlock</code>，但是回收的前提是这个 block 的<code class="language-plaintext highlighter-rouge">ref_count</code>变量值为 0，表示这个 block 内的 token kv cache 数据不再需要了。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockAllocator</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
      <span class="n">self</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
      <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
      <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
      <span class="n">self</span><span class="p">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks</span>

      <span class="c1"># Initialize the free blocks.
</span>      <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="nc">PhysicalTokenBlock</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                     <span class="n">block_number</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                                     <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PhysicalTokenBlock</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Out of memory! No free blocks are available.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">block</span>

  <span class="k">def</span> <span class="nf">free</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">PhysicalTokenBlock</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Double free! </span><span class="si">{</span><span class="n">block</span><span class="si">}</span><span class="s"> is already freed.</span><span class="sh">"</span><span class="p">)</span>
      <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_num_free_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">free_blocks</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h2 id="33--block-管理和映射模块">3.3 Block 管理和映射模块</h2> <p>在介绍这个Block 管理模块之前，我们先了解 vLLM 中设置的用来判断句子是否能够被分配物理 Block 的三种状态，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">AllocStatus</span><span class="p">(</span><span class="n">enum</span><span class="p">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Result for BlockSpaceManager.can_allocate
    </span><span class="sh">"""</span>
    <span class="n">OK</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">LATER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
    <span class="n">NEVER</span> <span class="o">=</span> <span class="n">enum</span><span class="p">.</span><span class="nf">auto</span><span class="p">()</span>
</code></pre></div></div> <p>三种状态的含义如下：</p> <ul> <li><code class="language-plaintext highlighter-rouge">OK</code>: seq_group 可以现在被分配。</li> <li><code class="language-plaintext highlighter-rouge">LATER</code>: seq_group 不能被分配。分配器的容量大于 seq_group 所需。</li> <li><code class="language-plaintext highlighter-rouge">NEVER</code>: seq_group 永远不能被分配。seq_group 太大，无法在 GPU 中分配。</li> </ul> <p><code class="language-plaintext highlighter-rouge">vllm/vllm/core/block_manager.py</code>下的<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>是一个高级内存管理器，它在内存密集型计算任务（尤其是在使用GPU和CPU进行大规模数据处理的情况下）中管理逻辑数据块和物理内存块之间的映射。</p> <p>接下来，我们结合代码介绍<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>一些重要的函数。</p> <ul> <li>初始化函数<code class="language-plaintext highlighter-rouge">__init__</code>: <ul> <li><code class="language-plaintext highlighter-rouge">watermark</code>: 一种阈值机制，用来决定何时停止在GPU上分配新的块，以避免内存不足</li> <li><code class="language-plaintext highlighter-rouge">watermark_blocks</code>: 计算出在达到内存不足前，还能在GPU上分配多少个块。</li> <li><code class="language-plaintext highlighter-rouge">sliding_window</code>: 可选参数，用来限制在任意给定时间内活跃的逻辑块的数量，有助于控制内存使用。</li> <li>创建了 cpu 和 gpu 两种 <code class="language-plaintext highlighter-rouge">BlockAllocator</code>,不过需要注意这里都是物理层面的 Block</li> <li>创建了一个字典 <code class="language-plaintext highlighter-rouge">block_tables</code>，用于存储每个 sequence id 和它所使用的物理块之间的映射。通过这个 sequence id ，我们就能找到对应的前面介绍的<code class="language-plaintext highlighter-rouge">Sequence</code>实例化对象，通过这个字典，就建立了逻辑 block 和物理 block 的映射关系。</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vllm/vllm/core/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_gpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_cpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">watermark</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">=</span> <span class="n">num_gpu_blocks</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_total_cpu_blocks</span> <span class="o">=</span> <span class="n">num_cpu_blocks</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">sliding_window</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">sliding_window</span><span class="p">,</span>
                                                      <span class="n">block_size</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="o">=</span> <span class="n">sliding_window</span> <span class="o">//</span> <span class="n">block_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark</span> <span class="o">=</span> <span class="n">watermark</span>
        <span class="k">assert</span> <span class="n">watermark</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>

        <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">watermark</span> <span class="o">*</span> <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">GPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_gpu_blocks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cpu_allocator</span> <span class="o">=</span> <span class="nc">BlockAllocator</span><span class="p">(</span><span class="n">Device</span><span class="p">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span>
                                            <span class="n">num_cpu_blocks</span><span class="p">)</span>
        <span class="c1"># Mapping: seq_id -&gt; BlockTable.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">BlockTable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">can_allocate</code> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="n">allocated</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">-=</span> <span class="n">seq_group</span><span class="p">.</span><span class="n">prefix</span><span class="p">.</span><span class="nf">get_num_blocks</span><span class="p">()</span>

      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">num_required_blocks</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">num_required_blocks</span><span class="p">,</span>
                                    <span class="n">self</span><span class="p">.</span><span class="n">block_sliding_window</span><span class="p">)</span>
      <span class="n">num_free_gpu_blocks</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">get_num_free_blocks</span><span class="p">()</span>

      <span class="c1"># Use watermark to avoid frequent cache eviction.
</span>      <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_total_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&lt;</span>
              <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">NEVER</span>
      <span class="k">if</span> <span class="n">num_free_gpu_blocks</span> <span class="o">-</span> <span class="n">num_required_blocks</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">watermark_blocks</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">OK</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">AllocStatus</span><span class="p">.</span><span class="n">LATER</span>
</code></pre></div> </div> </li> </ul> <p><code class="language-plaintext highlighter-rouge">can_allocate</code>方法用于判断一个序列组（<code class="language-plaintext highlighter-rouge">seq_group</code>）是否能被成功分配所需的内存块。此方法首先计算该序列组基于当前任务的逻辑数据块所需的总物理内存块数量。接着，它会检查GPU分配器中的空闲内存块数量，以确认是否有足够的资源满足需求。</p> <p>方法中引入了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>概念，其主要目的是防止因频繁进行内存块的缓存淘汰而影响系统性能。在模型训练或数据处理的动态环境中，内存需求持续变化，如果因缺乏足够的空闲内存块而不得不频繁淘汰并重新分配内存块，将会造成性能损耗。这是因为被淘汰的内存块很可能很快再次需要使用，其重新分配过程会消耗额外的时间和资源。</p> <p>通过设置<code class="language-plaintext highlighter-rouge">watermark_blocks</code>阈值，当GPU上的空闲内存块数量低于此阈值时，系统将避免分配新的内存块，以留出缓冲区域，减少缓存淘汰的发生。只有当空闲内存块数量高于此阈值时，系统才会继续进行新的内存块分配。这种策略旨在平衡内存分配需求和系统性能，避免因频繁的内存操作而降低效率。</p> <p>如果根据当前的资源状态，确定序列组所需的内存块永远无法被满足，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.NEVER</code>，意味着该序列组在当前条件下无法被分配。如果当前不可分配但未来有可能，返回<code class="language-plaintext highlighter-rouge">AllocStatus.LATER</code>，表明序列组暂时无法分配，但随着系统状态的改变，可能在将来能够分配。如果有足够的空闲内存块满足分配需求，则返回<code class="language-plaintext highlighter-rouge">AllocStatus.OK</code>，表示序列组可以立即被分配所需内存。</p> <p>这种方式确保了<code class="language-plaintext highlighter-rouge">watermark_blocks</code>在满足内存分配需求的同时，有效避免了频繁的缓存淘汰问题，从而优化了整体的系统性能和资源利用效率。</p> <ul> <li><code class="language-plaintext highlighter-rouge">allocate</code> 代码有简化，但是不影响理解 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BlockSpaceManager</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">seq</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">num_prompt_blocks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">logical_token_blocks</span><span class="p">)</span>

      <span class="n">block_table</span><span class="p">:</span> <span class="n">BlockTable</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">logical_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_prompt_blocks</span><span class="p">):</span>
          <span class="n">block</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_allocator</span><span class="p">.</span><span class="nf">allocate</span><span class="p">()</span>
          <span class="n">block</span><span class="p">.</span><span class="n">ref_count</span> <span class="o">=</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">num_seqs</span><span class="p">()</span>
          <span class="n">block_table</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="p">.</span><span class="nf">get_seqs</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">SequenceStatus</span><span class="p">.</span><span class="n">WAITING</span><span class="p">):</span>
          <span class="n">self</span><span class="p">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">seq</span><span class="p">.</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">block_table</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
</code></pre></div> </div> <p><code class="language-plaintext highlighter-rouge">allocate</code> 方法用于为序列组分配内存块。它会遍历序列组中的每个序列，为每个序列分配足够的内存块，并将这些块添加到序列的块表中。同时，它会更新序列的块表，以便在后续的训练过程中可以正确地访问这些块。</p> </li> </ul> <p><code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>还有很多其它的函数，为了避免文章累赘，这里不做详细介绍。</p> <p>后面会继续写一篇 vLLM 的调度<code class="language-plaintext highlighter-rouge">Scheduler</code>模块的文章，对<code class="language-plaintext highlighter-rouge">BlockSpaceManager</code>更加详细地介绍。相信通过本篇文章，你应该能够对 vLLM 的 block 有一个清楚的了解了，如果还是不清楚，可以反复阅读直到清楚为止。</p> <h1 id="参考">参考</h1> <ul> <li>https://zhuanlan.zhihu.com/p/681018057</li> <li>https://zhuanlan.zhihu.com/p/656939628</li> <li>https://zhuanlan.zhihu.com/p/655561941</li> <li>https://zhuanlan.zhihu.com/p/658233994</li> <li>https://zhuanlan.zhihu.com/p/641999400</li> </ul> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="LLM"/><category term="Serving"/><category term="vLLM"/><category term="大模型推理"/><summary type="html"><![CDATA[1. Block 概览]]></summary></entry><entry><title type="html">大模型推理框架 vLLM 源码解析（一） - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-marsggbo/" rel="alternate" type="text/html" title="大模型推理框架 vLLM 源码解析（一） - marsggbo"/><published>2024-02-04T10:15:00+00:00</published><updated>2024-02-04T10:15:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. Quick Start 创建如下代码，命名为 run.py from vllm import LLM, SamplingParams prompts=[ &quot;Have you followed marsggbo in Zhihu?&quot;, &quot;你一键三连了吗？&quot; ] # 输入prompts sam]]></summary></entry><entry><title type="html">vllm 安装踩坑 (The NVIDIA driver on your system is too old) - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-the-nvidia-driver-on-your-system-is-too-old-marsggbo/" rel="alternate" type="text/html" title="vllm 安装踩坑 (The NVIDIA driver on your system is too old) - marsggbo"/><published>2024-01-15T12:35:00+00:00</published><updated>2024-01-15T12:35:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/vllm--the-nvidia-driver-on-your-system-is-too-old---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-the-nvidia-driver-on-your-system-is-too-old-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[我的环境如下： nvidia-smi 显示 cuda 版本是 11.7 目前最新vllm 要求的 torch 版本是 2.1.2，该版本要求的 cuda 版本是 11.8，所以不匹配。执行安装会遇到如下错误 RuntimeError: The NVIDIA driver on your system]]></summary></entry><entry><title type="html">IEEE 浮点数表示原理 - marsggbo</title><link href="https://marsggbo.github.io/blog/2023/ieee-marsggbo/" rel="alternate" type="text/html" title="IEEE 浮点数表示原理 - marsggbo"/><published>2023-12-16T08:25:00+00:00</published><updated>2023-12-16T08:25:00+00:00</updated><id>https://marsggbo.github.io/blog/2023/ieee----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2023/ieee-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[原文： https://zhuanlan.zhihu.com/p/144697348]]></summary></entry><entry><title type="html">LLM 学习笔记-Deepspeed-MoE 论文 - marsggbo</title><link href="https://marsggbo.github.io/blog/2023/llm-deepspeed-moe-marsggbo/" rel="alternate" type="text/html" title="LLM 学习笔记-Deepspeed-MoE 论文 - marsggbo"/><published>2023-12-07T09:33:00+00:00</published><updated>2023-12-07T09:33:00+00:00</updated><id>https://marsggbo.github.io/blog/2023/llm--deepspeed-moe----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2023/llm-deepspeed-moe-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[论文 DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale 1. Introduction 现有的 MoE 方法在正式使用场景中存在的挑战： 场景局限：]]></summary></entry><entry><title type="html">AttributeError: module ‘torch’ has no attribute ‘fx’解决办法 - marsggbo</title><link href="https://marsggbo.github.io/blog/2023/attributeerror-module-torch-has-no-attribute-fx-marsggbo/" rel="alternate" type="text/html" title="AttributeError: module ‘torch’ has no attribute ‘fx’解决办法 - marsggbo"/><published>2023-12-02T10:23:00+00:00</published><updated>2023-12-02T10:23:00+00:00</updated><id>https://marsggbo.github.io/blog/2023/attributeerror-module-torch-has-no-attribute-fx---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2023/attributeerror-module-torch-has-no-attribute-fx-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[这个报错原因很好解决，只需要在引入包的时候调用下面的语句就可以了 import torch.fx]]></summary></entry><entry><title type="html">LLM 学习笔记-transformers库的 PreTrainedModel 和 ModelOutput 到底是什么？ - marsggbo</title><link href="https://marsggbo.github.io/blog/2023/llm-transformers-pretrainedmodel-modeloutput-marsggbo/" rel="alternate" type="text/html" title="LLM 学习笔记-transformers库的 PreTrainedModel 和 ModelOutput 到底是什么？ - marsggbo"/><published>2023-12-02T04:52:00+00:00</published><updated>2023-12-02T04:52:00+00:00</updated><id>https://marsggbo.github.io/blog/2023/llm--transformers-pretrainedmodel--modeloutput----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2023/llm-transformers-pretrainedmodel-modeloutput-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[闲言碎语 我在刚开始接触 huggingface （后简称 hf） 的 transformers 库时候感觉很冗杂，比如就模型而言，有 PretrainedModel, AutoModel，还有各种 ModelForClassification, ModelForCausalLM, AutoMode]]></summary></entry></feed>