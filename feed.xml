<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://marsggbo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://marsggbo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-29T03:10:51+00:00</updated><id>https://marsggbo.github.io/feed.xml</id><title type="html">HE Xin (贺鑫), a.k.a. MARSGGBO</title><subtitle>&quot;SSS: Study, Sleep, Slim&quot; </subtitle><entry><title type="html">Deepspeed ZeRO系列算法原理+通信开销详解 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/deepspeed-zero-marsggbo/" rel="alternate" type="text/html" title="Deepspeed ZeRO系列算法原理+通信开销详解 - marsggbo"/><published>2024-06-26T15:39:00+00:00</published><updated>2024-06-26T15:39:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/deepspeed-zero---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/deepspeed-zero-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[原文: https://sumanthrh.com/post/distributed-and-efficient-finetuning/#zero-powered-data-parallelism]]></summary></entry><entry><title type="html">NSCC集群使用笔记 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/nscc-marsggbo/" rel="alternate" type="text/html" title="NSCC集群使用笔记 - marsggbo"/><published>2024-06-09T13:19:00+00:00</published><updated>2024-06-09T13:19:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/nscc---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/nscc-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. 账号申请 如果是 NUS，NTU 或者 ASTAR 的学生，可以直接用自己的学校 ID 登录。登录不上的话可以发邮件联系 nscc 工作人员即可，基本上第二天就会回复解决。 2. VSCode 连接 账号申请下来后进官网设置你的 ssh key 之类的东西就可以登录了。第一次登录成功后，可以参]]></summary></entry><entry><title type="html">Huggingface Transformers实现张量并行的小坑 set/get_output_embeddings - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/huggingface-transformers-setget_output_embeddings-marsggbo/" rel="alternate" type="text/html" title="Huggingface Transformers实现张量并行的小坑 set/get_output_embeddings - marsggbo"/><published>2024-05-06T03:17:00+00:00</published><updated>2024-05-06T03:17:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/huggingface-transformers-setget_output_embeddings---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/huggingface-transformers-setget_output_embeddings-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[transformers 库里实现的很多模型会有这么两个函数 get_output_embeddings和 get_output_embeddings。以 SwitchTransformer 为例 class SwitchTransformersForConditionalGeneration(Sw]]></summary></entry><entry><title type="html">Pytorch 如何使用 storage 实现参数 offload？ - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/pytorch-storage-offload-marsggbo/" rel="alternate" type="text/html" title="Pytorch 如何使用 storage 实现参数 offload？ - marsggbo"/><published>2024-04-21T11:08:00+00:00</published><updated>2024-04-21T11:08:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/pytorch--storage--offload---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/pytorch-storage-offload-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[在深入探讨 PyTorch 中的 Storage 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 Stor]]></summary></entry><entry><title type="html">TACC 集群使用笔记</title><link href="https://marsggbo.github.io/blog/2024/TACC-%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/" rel="alternate" type="text/html" title="TACC 集群使用笔记"/><published>2024-04-10T16:40:16+00:00</published><updated>2024-04-10T16:40:16+00:00</updated><id>https://marsggbo.github.io/blog/2024/TACC%20%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/TACC-%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"><![CDATA[<h1 id="1注册账号">1注册账号</h1> <p>现在网页上注册账号，之后需要联系导师或者管理员把你添加到对应的集群里去，例如我加入的是 Lonestar6 集群。</p> <p>之后需要跟着这个<a href="https://docs.tacc.utexas.edu/basics/mfa/">教程</a>绑定 MFA 软件（可以是 DUO 或者 1password）</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_5_1707138200561.png" alt="MFA 绑定"/></p> <p>之后登录账号,系统会要求先后输入你的账户密码和 MFA 的 6 位数 token</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">local</span>@username<span class="nv">$ </span>ssh username@ls6.tacc.utexas.edu
<span class="o">(</span>username@ls6.tacc.utexas.edu<span class="o">)</span> Password:
<span class="o">(</span>username@ls6.tacc.utexas.edu<span class="o">)</span> TACC Token Code:

login1.ls6<span class="o">(</span>22<span class="o">)</span><span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SCRATCH</span>/
login1.ls6<span class="o">(</span>23<span class="o">)</span><span class="err">$</span>
</code></pre></div></div> <p>密码都正确之后你会进入到 login 节点，在这里千万不能随意执行大规模的计算任务，因为很有可能会被封号。你需要使用 compute 节点执行计算任务。</p> <p><img src="https://raw.githubusercontent.com/marsggbo/PicBed/master/小书匠/2024_2_5_1707139131743.png" alt="Login 和 Compute 节点"/></p> <p>成功登入后，默认进入 login 节点下的 <code class="language-plaintext highlighter-rouge">/home</code>目录，一般而言我们需要进入 <code class="language-plaintext highlighter-rouge">/scratch </code>目录。大多数TACC HPC资源上挂载了三个文件系统：<code class="language-plaintext highlighter-rouge">$HOME</code>、<code class="language-plaintext highlighter-rouge">$WORK</code>、和<code class="language-plaintext highlighter-rouge">$SCRATCH</code>，以下是它们的区别、使用场景和注意事项的总结：</p> <table> <thead> <tr> <th>文件系统</th> <th>区别与特点</th> <th>使用场景</th> <th>注意事项</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">$HOME</code></td> <td>用于用户的个人文件和设置。</td> <td>存储cron作业、小脚本、环境设置。</td> <td>避免在<code class="language-plaintext highlighter-rouge">$HOME</code>中运行作业，用于常规文件管理而不是并行作业。</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">$WORK</code></td> <td>用于存储软件安装、原始数据集等。</td> <td>存储软件安装、原始数据集、作业脚本和模板。</td> <td>注意文件系统配额，接近配额可能导致文件系统压力。</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">$SCRATCH</code></td> <td>临时存储、I/O文件、作业文件等。</td> <td>运行I/O密集型作业，存储临时数据集。</td> <td>避免在<code class="language-plaintext highlighter-rouge">$SCRATCH</code>中存储长期数据，文件可能在十天未访问后被清理。不要在<code class="language-plaintext highlighter-rouge">$SCRATCH</code>运行长期作业，用于短暂的、I/O密集型的作业。</td> </tr> </tbody> </table> <p>总体而言，<code class="language-plaintext highlighter-rouge">$HOME</code> 适用于个人文件和设置，<code class="language-plaintext highlighter-rouge">$WORK</code>适用于存储软件和重要数据，而<code class="language-plaintext highlighter-rouge">$SCRATCH</code>适用于短暂的、I/O密集型的作业。</p> <p>从实操性的角度说，进入computing node 后，默认先进入的是<code class="language-plaintext highlighter-rouge">HOME</code>目录，在这里你可以先设置好 conda 环境。接着，假如你要运行一个 pytorch 代码，你需要<code class="language-plaintext highlighter-rouge">cd $SCRATCH</code> 才能使用 GPU，这个目录下运行代码保存的日志是临时的，你需要将重要文件备份到 <code class="language-plaintext highlighter-rouge">$WORK</code> 目录下。</p> <h1 id="2-交互式开发环境-idev">2. 交互式开发环境 idev</h1> <p><code class="language-plaintext highlighter-rouge">idev</code> 是一个用于在TACC（Texas Advanced Computing Center）集群上创建交互式计算环境的命令行工具，可以在计算节点上创建一个交互式会话，可以在其中执行串行、OpenMP并行或MPI并行的代码，就像在批处理作业中一样。。以下是关于 <code class="language-plaintext highlighter-rouge">idev</code> 的一些主要用法和选项的介绍：</p> <h2 id="21-idev--参数选项">2.1 <code class="language-plaintext highlighter-rouge">idev</code> 参数选项：</h2> <ul> <li><code class="language-plaintext highlighter-rouge">-A account_name</code>：设置账户名称（默认为 <code class="language-plaintext highlighter-rouge">-A use_default</code>）。</li> <li><code class="language-plaintext highlighter-rouge">-m minutes</code>：设置计算时间（默认为 30 分钟）。</li> <li><code class="language-plaintext highlighter-rouge">-n total_tasks</code>：设置总任务数。</li> <li><code class="language-plaintext highlighter-rouge">-N nodes</code>：设置节点数量。</li> <li><code class="language-plaintext highlighter-rouge">-tpn tpn</code>：设置每节点任务数。</li> <li><code class="language-plaintext highlighter-rouge">-p queue_name</code>：设置队列名称（默认为 <code class="language-plaintext highlighter-rouge">-p development</code>）。</li> <li><code class="language-plaintext highlighter-rouge">-R</code>：查找用户的预约。</li> <li><code class="language-plaintext highlighter-rouge">-r reservation_name</code>：请求使用特定的预约。</li> <li><code class="language-plaintext highlighter-rouge">-r none</code>：禁用预约检查。</li> <li><code class="language-plaintext highlighter-rouge">-E</code>：在作业开始时通知。</li> <li><code class="language-plaintext highlighter-rouge">-e email_address</code>：在作业开始时通过指定的电子邮件地址通知。</li> <li><code class="language-plaintext highlighter-rouge">-t hh:mm:ss</code>：设置计算时间（默认为 30 分钟）。</li> <li><code class="language-plaintext highlighter-rouge">-queues</code>：列出系统的队列。</li> <li><code class="language-plaintext highlighter-rouge">-pselect</code>：显示可选择的 Slurm 队列。</li> <li><code class="language-plaintext highlighter-rouge">-qselect</code>：显示可选择的 Slurm 队列。</li> <li><code class="language-plaintext highlighter-rouge">-- &lt;other SLURM options&gt;</code>：必须在所有 idev 选项之后使用，用于指定其他 Slurm 选项。</li> </ul> <h2 id="22-示例">2.2 示例</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 默认设置：1 节点，16 任务，30 分钟，使用默认账户</span>
idev

<span class="c"># 显示帮助信息</span>
idev <span class="nt">--help</span>

<span class="c"># 设置队列、时间和账户</span>
idev <span class="nt">-p</span> development <span class="nt">-m</span> 90 <span class="nt">-A</span> AB-ccviss

<span class="c"># 设置队列、时间、账户、节点和任务数</span>
idev <span class="nt">-p</span> normal <span class="nt">-t</span> 00:90:00 <span class="nt">-A</span> TG-STA123 <span class="nt">-N</span> 2 <span class="nt">-n</span> 16

<span class="c"># 显示可选择的 Slurm 队列</span>
idev <span class="nt">-pselect</span>

<span class="c"># 设置交互式会话的最长时间为2小时, 1个节点，4 个任务，请求在 development 队列中执行计算任务</span>
idev <span class="nt">-t</span> 02:00:00 <span class="nt">-N</span> 1 <span class="nt">-n</span> 4 <span class="nt">-p</span> development

</code></pre></div></div> <p>上面最后一个例子使用的是名为<code class="language-plaintext highlighter-rouge">development</code>的节点，你也可以先使用<code class="language-plaintext highlighter-rouge">sinfo</code>命令查看所有节点，然后手动设置成空闲的节点，例如：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>sinfo
gpu-a100          up   infinite      1 drain<span class="k">*</span> c317-003
gpu-a100          up   infinite     67  alloc c302-[001-004],c303-[001-004],c305-[001-002,004],c306-[002-004],c308-[001-004],c309-[001-004],c310-[001-004],c315-[001,003-016],c316-[001-002,007-016],c317-[001-002,004-008,010-016]
gpu-a100          up   infinite      5   idle c304-001,c305-003,c306-001,c316-003,c317-009
gpu-a100-dev      up   infinite      2  alloc c301-[001,004]
<span class="nv">$ </span>idev <span class="nt">-t</span> 02:00:00 <span class="nt">-N</span> 1 <span class="nt">-n</span> 4 <span class="nt">-p</span> gpu-a100-dev
</code></pre></div></div> <p>上面命令会自动申请一个空闲的<code class="language-plaintext highlighter-rouge">gpu-a100-dev</code>节点。</p> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="techniques"/><category term="TACC,"/><category term="集群"/><summary type="html"><![CDATA[1注册账号]]></summary></entry><entry><title type="html">Pytorch 使用 storage 实现 offload 参数示例</title><link href="https://marsggbo.github.io/blog/2024/Pytorch-%E4%BD%BF%E7%94%A8-storage-%E5%AE%9E%E7%8E%B0-offload-%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B/" rel="alternate" type="text/html" title="Pytorch 使用 storage 实现 offload 参数示例"/><published>2024-04-10T14:04:10+00:00</published><updated>2024-04-10T14:04:10+00:00</updated><id>https://marsggbo.github.io/blog/2024/Pytorch%20%E4%BD%BF%E7%94%A8%20storage%20%E5%AE%9E%E7%8E%B0%20offload%20%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/Pytorch-%E4%BD%BF%E7%94%A8-storage-%E5%AE%9E%E7%8E%B0-offload-%E5%8F%82%E6%95%B0%E7%A4%BA%E4%BE%8B/"><![CDATA[<p>在深入探讨 PyTorch 中的 <code class="language-plaintext highlighter-rouge">Storage</code> 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 <code class="language-plaintext highlighter-rouge">Storage</code>。</p> <h1 id="1-什么是-torchstorage">1. 什么是 <code class="language-plaintext highlighter-rouge">torch.Storage</code>?</h1> <p>在 PyTorch 中，<code class="language-plaintext highlighter-rouge">Storage</code> 是一种容纳数据的一维数组，它可以看作是一个底层的内存块，其中存储着特定类型的数据。与 <code class="language-plaintext highlighter-rouge">Tensor</code> 的关系非常紧密，实际上，每个 <code class="language-plaintext highlighter-rouge">Tensor</code> 都有一个与之关联的 <code class="language-plaintext highlighter-rouge">Storage</code> 对象。<code class="language-plaintext highlighter-rouge">Tensor</code> 提供了一个高维视图来操作存储在 <code class="language-plaintext highlighter-rouge">Storage</code> 中的数据。</p> <p><code class="language-plaintext highlighter-rouge">Storage</code> 的一个关键特性是它的数据排列是连续的，这使得数据可以迅速地在设备之间传输，例如从 CPU 到 GPU，省去了频繁索引的操作。此外，<code class="language-plaintext highlighter-rouge">Storage</code> 可以存在于不同的设备上，如 CPU 或 CUDA（GPU）。</p> <p>使用 storage 实现 offload 参数场景大致有如下：</p> <ul> <li> <p><strong>模型训练时的内存优化</strong>： 在深度学习模型训练过程中，特别是当使用的模型非常大，以至于单个 GPU 显存不足时，可以使用 offload 技术将部分数据暂时存储到 CPU 内存中，从而释放 GPU 显存用于计算。</p> </li> <li> <p><strong>数据预处理</strong>： 在进行大规模数据处理时，可以将不活跃的数据段 offload 到 CPU，以保持 GPU 资源用于执行高优先级的任务。</p> </li> <li> <p><strong>长期数据存储</strong>： 对于不需要频繁访问的大量数据，可以将其 offload 到 CPU 或其他存储系统，以减少昂贵的 GPU 存储资源的占用。</p> </li> </ul> <h1 id="2-理解-storage">2. 理解 <code class="language-plaintext highlighter-rouge">Storage</code></h1> <h2 id="21-简单例子">2.1 简单例子</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>
</code></pre></div></div> <p>输出结果如下，可以看到打印出来的结果符合预期，有三个浮点数，storage 的类型是 <code class="language-plaintext highlighter-rouge">torch.storage.TypedStorage</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mf">0.0</span>
 <span class="mf">1.0</span>
 <span class="mf">2.0</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">TypedStorage</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <p>更一般地，我们还能打印看看无类型的 storage 是什么样的</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_storage</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="n">_untyped_storage</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下，可以看到总共有 12 个整数，这是因为前面我们使用的数据类型是 float32，也就是说每个数由 4 个字节（bytes）表示。因为 变量 x 总共有 3 个数，所有它的 storage 总共有 12 个字节。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>这些值实际上是浮点数<code class="language-plaintext highlighter-rouge">0</code>、<code class="language-plaintext highlighter-rouge">1</code>、<code class="language-plaintext highlighter-rouge">2</code>在内存中的字节级表示。需要注意的是，上面输出结果并不是随机值，而是这些浮点数在 IEEE 754 标准下的二进制表达。我们可以逐个解释这些值如何来的。</p> <h2 id="22-浮点数的-ieee-754-表示">2.2 浮点数的 IEEE 754 表示</h2> <p>对于类型 <code class="language-plaintext highlighter-rouge">float32</code>（即单精度浮点数），每个数字占用 4 个字节（32位），具体编码方式为：</p> <ul> <li>1 位符号位（最高位）</li> <li>8 位指数位</li> <li>23 位尾数位</li> </ul> <p>在解释这些值之前，我们先了解一下计算机中的 <strong>小端序（Little Endian）</strong> 存储方式：在这种存储方式中，低位字节存放在内存的低地址端，高位字节存放在高地址端。</p> <p>以<code class="language-plaintext highlighter-rouge">Tensor[0., 1., 2.]</code> 为例，我们来看看这些值在内存中是如何表示的：</p> <ol> <li><strong>数字 0 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：全0（偏移量为127，因此全0表示指数-127）</li> <li>尾数位：全0</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">00000000 00000000 00000000 00000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 00</code></li> <li><strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 0 0</code></li> </ul> </li> <li><strong>数字 1 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：127（偏移后为0，<code class="language-plaintext highlighter-rouge">01111111</code>）</li> <li>尾数位：全0（因为1.0的尾数部分无需额外存储）</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">001111111 00000000000000000000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">3F 80 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 80 3F</code></li> <li><strong>上面结果转化成十进制表示</strong>： <code class="language-plaintext highlighter-rouge">0 0 128 63</code> (<code class="language-plaintext highlighter-rouge">80</code> 十六进制转十进制是 <code class="language-plaintext highlighter-rouge">128</code>，<code class="language-plaintext highlighter-rouge">3F</code> 转十进制是 <code class="language-plaintext highlighter-rouge">63</code>)</li> </ul> </li> <li><strong>数字 2 的浮点表示</strong>： <ul> <li>符号位：0</li> <li>指数位：128（偏移后为1，<code class="language-plaintext highlighter-rouge">10000000</code>）</li> <li>尾数位：全0（因为2.0的尾数部分也无需额外存储）</li> <li><strong>二进制表示</strong>：<code class="language-plaintext highlighter-rouge">010000000 00000000000000000000000</code></li> <li><strong>十六进制表示</strong>：<code class="language-plaintext highlighter-rouge">40 00 00 00</code></li> <li><strong>小端序下的字节表示</strong>：<code class="language-plaintext highlighter-rouge">00 00 00 40</code></li> </ul> </li> </ol> <h1 id="3-使用-storage-实现参数-offload-到-cpu">3. 使用 Storage 实现参数 offload 到 cpu</h1> <p>前面例子中的变量<code class="language-plaintext highlighter-rouge">x</code>在 cuda上，为了实现 offload，我们需要在 cpu 上创建一个 storage，如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">nbytes</span><span class="p">).</span><span class="nf">pin_memory</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下,可以看到<code class="language-plaintext highlighter-rouge">offload_storage</code>是在 cpu 上，目前其上面的值都是一些随机值。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">208</span>
 <span class="mi">238</span>
 <span class="mi">22</span>
 <span class="mi">7</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">208</span>
 <span class="mi">66</span>
 <span class="mi">20</span>
 <span class="mi">6</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>接下来我们需要把 <code class="language-plaintext highlighter-rouge">x</code> offload 到 cpu 上，只需要对 storage 做 copy 操作即可，代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offload_storage</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">x_storage</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">offload_storage</span><span class="p">)</span>
</code></pre></div></div> <p>输出结果如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">128</span>
 <span class="mi">63</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">64</span>
<span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">storage</span><span class="p">.</span><span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">cpu</span><span class="p">)</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p>可以看到<code class="language-plaintext highlighter-rouge">x</code>的值被成功拷贝到 cpu 上，但是这离实现 offload 还有一步之遥，我们接下来继续看一个简单的 offload 例子。</p> <h1 id="4-gpu-参数-和-cpu-参数互换">4. gpu 参数 和 cpu 参数互换</h1> <p>我们接着将探讨如何利用 Storage 实现 GPU 和 CPU 之间的数据互换，这对于处理大型数据集或进行复杂的数据处理任务时尤其有用。</p> <p>假设我们有以下设置：</p> <ul> <li>一个 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 用于当前计算。</li> <li>多个 CPU <code class="language-plaintext highlighter-rouge">Storage</code> 用于存储额外的数据集，这些数据集可能在不同时间被需求到 GPU。</li> </ul> <h2 id="41--初始化环境">4.1 初始化环境</h2> <p>首先，我们定义一个在 CUDA 上的 Tensor 和多个在 CPU 上的 Storage，准备用于数据交换：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># 定义 CUDA Tensors (用于当前计算)
</span><span class="n">current_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># 定义 CPU Storages (用于存储额外数据)
</span><span class="n">extra_data1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>
<span class="n">extra_data3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Initial CUDA Tensor (Current Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">current_data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Initial CPU Storages (Extra Data):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 1:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <p>输出结果为：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Initial</span> <span class="n">CUDA</span> <span class="nc">Tensor </span><span class="p">(</span><span class="n">Current</span> <span class="n">Data</span><span class="p">):</span>
<span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>

<span class="n">Initial</span> <span class="n">CPU</span> <span class="nc">Storages </span><span class="p">(</span><span class="n">Extra</span> <span class="n">Data</span><span class="p">):</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="42-使用缓冲区进行数据交换">4.2 使用缓冲区进行数据交换</h2> <p>接下来，我们将根据需要将 CPU 上的数据加载到 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 中，同时将当前 CUDA <code class="language-plaintext highlighter-rouge">Tensor</code> 的数据存储回某个 CPU <code class="language-plaintext highlighter-rouge">Storage</code>，这可以申请一个 buffer 来作为中间变量，反正数据丢失。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 缓冲区定义
</span><span class="n">cpu_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">size</span><span class="p">()).</span><span class="nf">storage</span><span class="p">().</span><span class="nf">pin_memory</span><span class="p">()</span>  <span class="c1"># CPU buffer storage
</span>
<span class="c1"># 场景1：将 current_data 保存到 extra_data1，从 extra_data1 加载新数据到 current_data
</span><span class="n">cpu_buffer</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">())</span>  <span class="c1"># Save current GPU data to CPU buffer
</span><span class="n">current_data</span><span class="p">.</span><span class="nf">storage</span><span class="p">().</span><span class="nf">copy_</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">)</span>  <span class="c1"># Move from CUDA buffer to current_data
</span><span class="n">extra_data1</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">cpu_buffer</span><span class="p">)</span>  <span class="c1"># Move from CPU buffer to extra_data1 Storage
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">After Data Exchange Scenario 1:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Current Data on </span><span class="si">{</span><span class="n">current_data</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">current_data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Updated Extra Data 1 on </span><span class="si">{</span><span class="n">extra_data1</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data1</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 2:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extra Data 3:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">list</span><span class="p">(</span><span class="n">extra_data3</span><span class="p">))</span>
</code></pre></div></div> <h4 id="输出结果">输出结果</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">After</span> <span class="n">Data</span> <span class="n">Exchange</span> <span class="n">Scenario</span> <span class="mi">1</span><span class="p">:</span>
<span class="n">Updated</span> <span class="n">Current</span> <span class="n">Data</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span> <span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">Updated</span> <span class="n">Extra</span> <span class="n">Data</span> <span class="mi">1</span> <span class="n">on</span> <span class="n">cpu</span><span class="p">:</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="n">Extra</span> <span class="n">Data</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]</span>
</code></pre></div></div> <p>此示例清晰地展示了如何利用 PyTorch 的 Storage 类来有效管理内存资源，并通过使用 CPU 和 CUDA 缓冲区动态切换数据来优化应用性能。这种方法尤其适用于需要频繁在不同计算设备之间迁移数据的场景，从而保证计算效率和响应速度。</p> <p>尽管可以通过 PyTorch 的 to(‘cpu’) 或 to(‘cuda’) 方法简单地在设备间迁移数据，使用 Storage 提供了更细粒度的控制。这在处理需要大量连续物理存储空间的复杂模型时显得尤为重要。</p> <p>例如在混合专家模型（MoE）中，系统需要根据不同的请求动态调用不同的专家（模型）。每个专家可能包含的是由多层感知机 (MLP) 或更复杂结构组成的模型，其中每层的参数在内存中通常是不连续的。这种不连续性可能导致在将参数 offload 到 CPU 或重新加载到 GPU 时，因频繁的内存访问和索引操作而增加通信开销。</p> <footer style="color:white;;background-color:rgb(24,24,24);padding:10px;border-radius:10px;"> <h3 style="text-align:center;color:tomato;font-size:16px;" id="autoid-2-0-0"> <center> <span>微信公众号：AutoML机器学习</span><br/> <img src="https://pic4.zhimg.com/80/v2-87083e55cd41dbef83cc840c142df48a_720w.jpeg" style="width:200px;height:200px"/> </center> <b>MARSGGBO</b><b style="color:white;"><span style="font-size:25px;">♥</span>原创</b><br/> <span>如有意合作或学术讨论欢迎私戳联系~<br/>邮箱:marsggbo@foxmail.com</span> <b style="color:white;"><br/> </b><p><b style="color:white;"></b> </p></h3> </footer>]]></content><author><name></name></author><category term="/techniques"/><category term="技术,pytorch,offload,torch.Storage"/><summary type="html"><![CDATA[在深入探讨 PyTorch 中的 Storage 类以及其在参数 offload 场景中的应用之前，让我们首先了解一下 PyTorch 和它的基础组件。PyTorch 是一个广泛使用的开源机器学习库，它不仅提供了强大的计算图功能和自动梯度计算，还允许开发者直接操作底层数据结构，这其中就包括 Storage。]]></summary></entry><entry><title type="html">TACC 集群使用笔记 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/tacc-marsggbo/" rel="alternate" type="text/html" title="TACC 集群使用笔记 - marsggbo"/><published>2024-04-10T06:26:00+00:00</published><updated>2024-04-10T06:26:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/tacc----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/tacc-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1注册账号 先在网页上注册账号，之后需要联系导师或者管理员把你添加到对应的集群里去，例如我加入的是 Lonestar6 集群。 之后需要跟着这个教程绑定 MFA 软件（可以是 DUO 或者 1password） 之后登录账号,系统会要求先后输入你的账户密码和 MFA 的 6 位数 token loc]]></summary></entry><entry><title type="html">图解 vLLM 的推理调度策略 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-marsggbo/" rel="alternate" type="text/html" title="图解 vLLM 的推理调度策略 - marsggbo"/><published>2024-04-04T02:32:00+00:00</published><updated>2024-04-04T02:32:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[原文： 从continuous batching到vLLM中的batching - 不知叫什么好的文章 - 知乎 https://zhuanlan.zhihu.com/p/688551989]]></summary></entry><entry><title type="html">大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/" rel="alternate" type="text/html" title="大模型推理框架 vLLM 源码解析（二）：Block 模块分配和管理 - marsggbo"/><published>2024-03-23T12:48:00+00:00</published><updated>2024-03-23T12:48:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/-vllm-block----marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/vllm-block-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[1. Block 概览 vLLM 的一个很大创新点是将物理层面的 GPU 和 CPU 可用内存切分成若干个 block,这样可以有效降低内存碎片化问题。具体而言，vLLM 的 block 分为逻辑层面（logical）和物理层面（physical），二者之间存在映射关系。下图很好解释了两个层面 bl]]></summary></entry><entry><title type="html">OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo</title><link href="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/" rel="alternate" type="text/html" title="OpenAI 的视频生成大模型Sora的核心技术详解（一）：Diffusion模型原理和代码详解 - marsggbo"/><published>2024-02-22T08:50:00+00:00</published><updated>2024-02-22T08:50:00+00:00</updated><id>https://marsggbo.github.io/blog/2024/openai-soradiffusion---marsggbo</id><content type="html" xml:base="https://marsggbo.github.io/blog/2024/openai-soradiffusion-marsggbo/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[标题党一下，顺便蹭一下 OpenAI Sora大模型的热点，主要也是回顾一下扩散模型的原理。 1. 简单理解扩散模型 简单理解，扩散模型如下图所示可以分成两部分，一个是 forward，另一个是 reverse 过程： forward：这是加噪声的过程，表示为\(q(X_{0:T})\)，即在原图（]]></summary></entry></feed>